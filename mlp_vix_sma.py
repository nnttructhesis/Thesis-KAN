# -*- coding: utf-8 -*-
"""MLP VIX SMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sa5u5CIi-gmhKWZHDZDiYS9zmJelyeS8
"""

!pip install numpy pandas matplotlib torch torchkan pykan arch kan

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class ImprovedBSplineKAN(nn.Module):
    """Your original improved B-spline KAN layer with better numerical stability"""

    def __init__(self, in_features: int, out_features: int, grid_size: int = 8,
                 spline_order: int = 3, scale_noise: float = 0.005):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        self.spline_order = spline_order

        # Grid points for B-splines - make them learnable for adaptivity
        h = 2.0 / grid_size
        grid = torch.linspace(-1 - h * spline_order, 1 + h * spline_order,
                             grid_size + 2 * spline_order + 1)
        self.register_parameter('grid', nn.Parameter(grid))

        # Learnable coefficients with better initialization
        self.coefficients = nn.Parameter(
            torch.randn(out_features, in_features, grid_size + spline_order) * scale_noise
        )

        # Base linear transformation for stability - make it stronger
        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * scale_noise)
        self.base_bias = nn.Parameter(torch.zeros(out_features))

        # Adaptive scale factor
        self.scale = nn.Parameter(torch.ones(out_features))

        # Input normalization
        self.input_norm = nn.LayerNorm(in_features)

    def b_splines(self, x: torch.Tensor) -> torch.Tensor:
        """Compute B-spline basis functions with improved stability"""
        assert x.dim() == 2 and x.size(1) == self.in_features

        # Apply input normalization first
        x = self.input_norm(x)

        # Clamp to avoid boundary issues with learnable bounds
        grid_min, grid_max = torch.min(self.grid), torch.max(self.grid)
        x = torch.clamp(x, grid_min + 0.01, grid_max - 0.01)

        # Sort grid to ensure monotonicity
        grid = torch.sort(self.grid)[0]

        bases = []

        for i in range(len(grid) - self.spline_order - 1):
            base = torch.ones_like(x)  # Start with degree 0

            for p in range(1, self.spline_order + 1):
                # Left term
                left_denom = grid[i + p] - grid[i] + 1e-8
                left = (x - grid[i]) / left_denom
                left = torch.where((x >= grid[i]) & (x < grid[i + p]), left, torch.zeros_like(x))

                # Right term
                right_denom = grid[i + p + 1] - grid[i + 1] + 1e-8
                right = (grid[i + p + 1] - x) / right_denom
                right = torch.where((x >= grid[i + 1]) & (x < grid[i + p + 1]), right, torch.zeros_like(x))

                # Combine with previous base
                new_base = torch.zeros_like(x)
                # Only apply where the base is active
                mask = (x >= grid[i]) & (x < grid[i + self.spline_order + 1])
                new_base = torch.where(mask, left * base + right * base, torch.zeros_like(x))
                base = new_base

            bases.append(base)

        return torch.stack(bases, dim=-1)  # [batch, in_features, num_bases]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Compute B-spline bases
        bases = self.b_splines(x)  # [batch, in_features, num_bases]

        # Apply spline transformation
        spline_output = torch.zeros(x.size(0), self.out_features, device=x.device)

        for i in range(self.out_features):
            for j in range(self.in_features):
                spline_output[:, i] += torch.sum(
                    bases[:, j, :] * self.coefficients[i, j, :], dim=-1
                )

        # Add stronger base linear transformation
        linear_output = F.linear(x, self.base_weight, self.base_bias)

        # Combine with learnable mixing (increased linear component)
        output = self.scale * (spline_output + 0.3 * linear_output)

        return output

class EnhancedVolatilityKAN(nn.Module):
    """Enhanced KAN architecture using your original B-spline implementation"""

    def __init__(self, input_dim: int, hidden_dims: list = [128, 64, 32],
                 output_dim: int = 1, dropout_rate: float = 0.2,
                 use_skip_connections: bool = True):
        super().__init__()

        self.use_skip_connections = use_skip_connections
        dims = [input_dim] + hidden_dims + [output_dim]

        # KAN layers with progressive grid refinement
        self.kan_layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()
        self.layer_norms = nn.ModuleList()
        self.activations = nn.ModuleList()

        grid_sizes = [8, 6, 4]  # Decreasing grid size for deeper layers

        for i in range(len(dims) - 1):
            grid_size = grid_sizes[min(i, len(grid_sizes) - 1)]

            self.kan_layers.append(
                ImprovedBSplineKAN(
                    dims[i], dims[i+1],
                    grid_size=grid_size,
                    spline_order=3,
                    scale_noise=0.005
                )
            )

            if i < len(dims) - 2:  # No dropout/norm on output layer
                self.dropouts.append(nn.Dropout(dropout_rate))
                self.layer_norms.append(nn.LayerNorm(dims[i+1]))
                self.activations.append(nn.SiLU())

        # Skip connections for deeper layers
        if self.use_skip_connections and len(hidden_dims) > 1:
            self.skip_connections = nn.ModuleList()
            for i in range(1, len(hidden_dims)):
                if dims[i] == dims[i+1]:  # Same dimension for skip
                    self.skip_connections.append(nn.Identity())
                else:
                    self.skip_connections.append(nn.Linear(dims[i], dims[i+1]))

        # Final output processing
        self.output_activation = nn.Softplus(beta=2.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        skip_outs = []

        for i, layer in enumerate(self.kan_layers[:-1]):  # All but last layer
            x = layer(x)

            if i < len(self.layer_norms):
                x = self.layer_norms[i](x)
                x = self.activations[i](x)
                x = self.dropouts[i](x)

            # Store for skip connections
            if self.use_skip_connections:
                skip_outs.append(x)

                # Apply skip connection if available
                if i > 0 and i-1 < len(self.skip_connections):
                    skip_input = self.skip_connections[i-1](skip_outs[i-1])
                    x = x + 0.1 * skip_input  # Weighted skip connection

        # Final layer
        if len(self.kan_layers) > 0:
            x = self.kan_layers[-1](x)

        # Output processing
        x = self.output_activation(x)

        return x

class KAN_SMA_Model(nn.Module):
    """Enhanced KAN model with SMA features"""

    def __init__(self, base_features: int, sma_features: int,
                 hidden_dims: list = [96, 64, 32], output_dim: int = 1):
        super().__init__()

        # Separate processing for base and SMA features
        self.base_processor = nn.Sequential(
            nn.Linear(base_features, 64),
            nn.LayerNorm(64),
            nn.SiLU(),
            nn.Dropout(0.1)
        )

        self.sma_processor = nn.Sequential(
            nn.Linear(sma_features, 32),
            nn.LayerNorm(32),
            nn.SiLU(),
            nn.Dropout(0.1)
        )

        # Attention mechanism for feature fusion
        self.attention = nn.MultiheadAttention(
            embed_dim=96, num_heads=4, dropout=0.1, batch_first=True
        )

        # Enhanced KAN processing
        self.kan_core = EnhancedVolatilityKAN(
            input_dim=96,  # 64 + 32
            hidden_dims=hidden_dims,
            output_dim=output_dim,
            dropout_rate=0.2,
            use_skip_connections=True
        )

    def forward(self, base_features: torch.Tensor, sma_features: torch.Tensor) -> torch.Tensor:
        # Process features separately
        base_proc = self.base_processor(base_features)
        sma_proc = self.sma_processor(sma_features)

        # Combine features
        combined = torch.cat([base_proc, sma_proc], dim=-1)

        # Apply attention (treating each sample as a sequence of length 1)
        combined_expanded = combined.unsqueeze(1)  # [batch, 1, features]
        attn_out, _ = self.attention(combined_expanded, combined_expanded, combined_expanded)
        combined = attn_out.squeeze(1)  # [batch, features]

        # KAN processing
        output = self.kan_core(combined)

        return output

class KAN_VIX_Model(nn.Module):
    """Enhanced KAN model with VIX features and market regime detection"""

    def __init__(self, base_features: int, vix_features: int,
                 hidden_dims: list = [128, 64, 32], output_dim: int = 1):
        super().__init__()

        # VIX regime detection
        self.vix_regime_detector = nn.Sequential(
            nn.Linear(vix_features, 16),
            nn.Tanh(),
            nn.Linear(16, 4),  # Low, Normal, High, Extreme volatility regimes
            nn.Softmax(dim=-1)
        )

        # Base feature processing
        self.base_processor = nn.Sequential(
            nn.Linear(base_features, 80),
            nn.LayerNorm(80),
            nn.GELU(),
            nn.Dropout(0.15)
        )

        # VIX feature processing with regime conditioning
        self.vix_processor = nn.Sequential(
            nn.Linear(vix_features + 4, 48),  # +4 for regime probabilities
            nn.LayerNorm(48),
            nn.GELU(),
            nn.Dropout(0.15)
        )

        # Cross-attention between base and VIX features
        self.cross_attention = nn.Sequential(
            nn.Linear(128, 64),  # 80 + 48
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Sigmoid()
        )

        # Enhanced KAN with regime-aware processing
        self.kan_core = EnhancedVolatilityKAN(
            input_dim=128,
            hidden_dims=hidden_dims,
            output_dim=output_dim,
            dropout_rate=0.2,
            use_skip_connections=True
        )

    def forward(self, base_features: torch.Tensor, vix_features: torch.Tensor) -> torch.Tensor:
        # Detect VIX regime
        regime_probs = self.vix_regime_detector(vix_features)

        # Process base features
        base_proc = self.base_processor(base_features)

        # Process VIX features with regime information
        vix_with_regime = torch.cat([vix_features, regime_probs], dim=-1)
        vix_proc = self.vix_processor(vix_with_regime)

        # Combine features
        combined = torch.cat([base_proc, vix_proc], dim=-1)

        # Apply cross-attention
        attention_weights = self.cross_attention(combined)
        combined = combined * attention_weights

        # KAN processing
        output = self.kan_core(combined)

        return output

class OriginalMLP(nn.Module):
    """Original simple MLP baseline for comparison"""

    def __init__(self, input_dim: int, hidden_dims: list = [64, 32, 16],
                 output_dim: int = 1, dropout_rate: float = 0.3):
        super().__init__()

        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()

        for i in range(len(dims) - 1):
            self.layers.append(nn.Linear(dims[i], dims[i+1]))

            if i < len(dims) - 2:  # No dropout on output layer
                self.dropouts.append(nn.Dropout(dropout_rate))

        # Output activation for positive volatility
        self.output_activation = nn.Softplus()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x)
            x = F.tanh(x)  # Simple tanh activation
            x = self.dropouts[i](x)

        # Final layer
        x = self.layers[-1](x)
        x = self.output_activation(x)

        return x

def calculate_technical_indicators(returns: np.ndarray, prices: np.ndarray) -> dict:
    """Calculate various technical indicators"""
    indicators = {}

    # RSI
    gains = np.where(returns > 0, returns, 0)
    losses = np.where(returns < 0, -returns, 0)
    avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)
    avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)
    rs = avg_gain / (avg_loss + 1e-8)
    indicators['rsi'] = 100 - (100 / (1 + rs))

    # Bollinger Bands position
    if len(prices) >= 20:
        sma20 = np.mean(prices[-20:])
        std20 = np.std(prices[-20:])
        bb_position = (prices[-1] - sma20) / (2 * std20 + 1e-8)
        indicators['bb_position'] = np.clip(bb_position, -2, 2)
    else:
        indicators['bb_position'] = 0

    # Price momentum
    if len(prices) >= 10:
        indicators['momentum_10'] = (prices[-1] - prices[-10]) / (prices[-10] + 1e-8)
    else:
        indicators['momentum_10'] = 0

    return indicators

def calculate_vix_features_from_file(vix_data: pd.DataFrame, current_date_idx: int) -> list:
    """Calculate VIX features from actual VIX CSV file"""
    vix_features = []

    # Check if VIX data is available
    if len(vix_data) == 0:
        return [20.0, 5.0, 0.0, 1.0, 0.0]  # Default values if no VIX data

    # Determine VIX column name
    if 'VIX' in vix_data.columns:
        vix_col = 'VIX'
    elif 'Close' in vix_data.columns:
        vix_col = 'Close'
    elif 'Adj Close' in vix_data.columns:
        vix_col = 'Adj Close'
    else:
        return [20.0, 5.0, 0.0, 1.0, 0.0]  # Default values if VIX column not found

    # Use available VIX data (limit to current date index)
    end_idx = min(current_date_idx, len(vix_data) - 1)
    start_idx = max(0, end_idx - 30)  # Use last 30 VIX observations

    vix_values = vix_data[vix_col].iloc[start_idx:end_idx+1].values

    if len(vix_values) == 0:
        return [20.0, 5.0, 0.0, 1.0, 0.0]

    # Current VIX level
    current_vix = vix_values[-1] if not np.isnan(vix_values[-1]) else 20.0
    vix_features.append(current_vix)

    # VIX volatility (volatility of VIX itself)
    if len(vix_values) > 5:
        vix_returns = np.diff(np.log(vix_values + 1e-8))
        vix_vol = np.std(vix_returns) * np.sqrt(252)
    else:
        vix_vol = 5.0
    vix_features.append(vix_vol)

    # VIX momentum (recent change)
    if len(vix_values) >= 5:
        vix_momentum = (vix_values[-1] - vix_values[-5]) / (vix_values[-5] + 1e-8)
    else:
        vix_momentum = 0.0
    vix_features.append(vix_momentum)

    # VIX relative level (current vs recent average)
    if len(vix_values) >= 20:
        vix_avg = np.mean(vix_values[-20:])
        vix_relative = vix_values[-1] / (vix_avg + 1e-8)
    else:
        vix_relative = 1.0
    vix_features.append(vix_relative)

    # VIX term structure (if we have enough data)
    if len(vix_values) >= 10:
        short_term_vix = np.mean(vix_values[-5:])
        long_term_vix = np.mean(vix_values[-20:]) if len(vix_values) >= 20 else np.mean(vix_values)
        term_structure = short_term_vix / (long_term_vix + 1e-8)
    else:
        term_structure = 1.0
    vix_features.append(term_structure)

    return vix_features

def create_enhanced_volatility_features(data: pd.DataFrame, vix_data: pd.DataFrame = None,
                                      lookback: int = 30, vol_window: int = 30,
                                      include_sma: bool = False,
                                      include_vix: bool = False) -> dict:
    """Create enhanced volatility features with optional SMA and real VIX data"""

    # Basic volatility features
    price_col = 'Adjusted' if 'Adjusted' in data.columns else 'Close'
    data = data.copy()
    data['Returns'] = data[price_col].pct_change()
    data['LogReturns'] = np.log(data[price_col]).diff()

    # Rolling volatility
    col_name = f'RollingVol_{vol_window}'
    data[col_name] = data['LogReturns'].rolling(window=vol_window, min_periods=vol_window//2).std() * np.sqrt(252)
    data['RollingVol'] = data[col_name]
    data.dropna(inplace=True)

    features = []
    sma_features = []
    vix_features = []
    targets = []

    min_start = max(lookback, vol_window)

    for i in range(min_start, len(data)):
        # Historical data window
        hist_returns = data['LogReturns'].iloc[i-lookback:i].values
        hist_prices = data[price_col].iloc[i-lookback:i].values
        hist_vol = data[col_name].iloc[i-lookback:i].values

        # Enhanced base features
        feature_vector = []

        # Multi-scale volatility features
        for window in [5, 10, 15, 20]:
            if len(hist_vol) >= window:
                vol_short = np.nanmean(hist_vol[-window:])
                feature_vector.append(vol_short)
            else:
                feature_vector.append(0)

        # Volatility regime features
        current_vol = hist_vol[-1] if not np.isnan(hist_vol[-1]) else 0
        vol_percentile = np.sum(hist_vol <= current_vol) / len(hist_vol) if len(hist_vol) > 1 else 0.5
        feature_vector.extend([current_vol, vol_percentile])

        # Return distribution features (enhanced)
        return_mean = np.mean(hist_returns)
        return_std = np.std(hist_returns)
        return_skew = np.mean(((hist_returns - return_mean) / (return_std + 1e-8))**3)
        return_kurt = np.mean(((hist_returns - return_mean) / (return_std + 1e-8))**4)
        feature_vector.extend([return_mean, return_std, return_skew, return_kurt])

        # Extreme value features
        downside_returns = hist_returns[hist_returns < 0]
        upside_returns = hist_returns[hist_returns > 0]
        downside_vol = np.std(downside_returns) if len(downside_returns) > 0 else 0
        upside_vol = np.std(upside_returns) if len(upside_returns) > 0 else 0
        asymmetry = downside_vol / (upside_vol + 1e-8)
        feature_vector.append(asymmetry)

        # Technical indicators (enhanced)
        tech_indicators = calculate_technical_indicators(hist_returns, hist_prices)
        feature_vector.extend([
            tech_indicators['rsi'] / 100.0,
            tech_indicators['bb_position'],
            tech_indicators['momentum_10']
        ])

        # Volatility clustering and persistence
        vol_autocorr = np.corrcoef(hist_vol[1:], hist_vol[:-1])[0,1] if len(hist_vol) > 1 else 0
        vol_autocorr = 0 if np.isnan(vol_autocorr) else vol_autocorr
        squared_returns = hist_returns**2
        vol_clustering = np.corrcoef(squared_returns[1:], squared_returns[:-1])[0,1] if len(hist_returns) > 1 else 0
        vol_clustering = 0 if np.isnan(vol_clustering) else vol_clustering
        feature_vector.extend([vol_autocorr, vol_clustering])

        # Non-linear volatility transformations (perfect for KAN)
        log_vol = np.log(current_vol + 1e-8)
        sqrt_vol = np.sqrt(current_vol)
        sq_vol = current_vol ** 2
        tanh_vol = np.tanh(current_vol * 10)
        feature_vector.extend([log_vol, sqrt_vol, sq_vol, tanh_vol])

        # Interaction features (KAN excels at these)
        if len(hist_returns) >= 10:
            vol_short = np.std(hist_returns[-5:]) * np.sqrt(252)
            vol_long = np.std(hist_returns[-10:]) * np.sqrt(252)
            vol_ratio = vol_short / (vol_long + 1e-8)
            vol_diff = vol_short - vol_long
            feature_vector.extend([vol_ratio, vol_diff])
        else:
            feature_vector.extend([1.0, 0.0])

        # Higher-order moments
        if len(hist_returns) > 0:
            returns_norm = (hist_returns - np.mean(hist_returns)) / (np.std(hist_returns) + 1e-8)
            fifth_moment = np.mean(returns_norm ** 5)
            feature_vector.append(fifth_moment)
        else:
            feature_vector.append(0)

        features.append(feature_vector)

        # SMA features if requested
        if include_sma:
            sma_feat = []
            for sma_window in [5, 10, 20, 50]:
                if i >= sma_window:
                    sma = np.mean(hist_prices[-sma_window:])
                    sma_ratio = hist_prices[-1] / sma
                    sma_slope = (sma - np.mean(hist_prices[-sma_window-5:-5])) / 5 if i >= sma_window + 5 else 0
                    sma_feat.extend([sma_ratio, sma_slope])
                else:
                    sma_feat.extend([1.0, 0.0])

            # SMA crossover signals
            if i >= 50:
                sma_fast = np.mean(hist_prices[-10:])
                sma_slow = np.mean(hist_prices[-50:])
                crossover = (sma_fast / sma_slow - 1) * 100
                sma_feat.append(crossover)
            else:
                sma_feat.append(0.0)

            sma_features.append(sma_feat)

        # Real VIX features if requested
        if include_vix and vix_data is not None:
            vix_feat = calculate_vix_features_from_file(vix_data, i)
            vix_features.append(vix_feat)

        targets.append(data['RollingVol'].iloc[i])

    result = {
        'base_features': np.array(features),
        'targets': np.array(targets)
    }

    if include_sma:
        result['sma_features'] = np.array(sma_features)
    if include_vix:
        result['vix_features'] = np.array(vix_features)

    return result

def train_model_with_advanced_techniques(model, X_train, y_train, X_test, y_test,
                                       optimizer, scheduler, epochs=200, batch_size=64):
    """Advanced training with multiple techniques"""

    criterion = nn.MSELoss()
    dataset = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
    )
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    best_loss = float('inf')
    patience = 0
    patience_limit = 30

    # Learning rate warmup
    warmup_epochs = 10
    base_lr = optimizer.param_groups[0]['lr']

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        # Learning rate warmup
        if epoch < warmup_epochs:
            lr = base_lr * (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            pred = model(batch_X)
            loss = criterion(pred, batch_y)

            # L1 regularization for sparsity
            l1_loss = 0
            for param in model.parameters():
                l1_loss += torch.sum(torch.abs(param))
            loss += 1e-6 * l1_loss

            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            epoch_loss += loss.item()

        # Validation
        model.eval()
        with torch.no_grad():
            test_pred = model(torch.tensor(X_test, dtype=torch.float32))
            test_loss = criterion(test_pred, torch.tensor(y_test, dtype=torch.float32).unsqueeze(1))

        if epoch >= warmup_epochs:
            scheduler.step()

        # Early stopping
        if test_loss < best_loss:
            best_loss = test_loss
            patience = 0
        else:
            patience += 1

        if patience >= patience_limit:
            print(f"Early stopping at epoch {epoch}")
            break

        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Train={epoch_loss/len(dataloader):.6f}, Test={test_loss:.6f}")

    return model

def train_dual_input_model(model, X1_train, X2_train, y_train, X1_test, X2_test, y_test,
                          optimizer, scheduler, epochs=200, batch_size=64):
    """Training function for dual input models (KAN+SMA, KAN+VIX)"""

    criterion = nn.MSELoss()
    dataset = TensorDataset(
        torch.tensor(X1_train, dtype=torch.float32),
        torch.tensor(X2_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
    )
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    best_loss = float('inf')
    patience = 0
    patience_limit = 30

    # Learning rate warmup
    warmup_epochs = 10
    base_lr = optimizer.param_groups[0]['lr']

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        # Learning rate warmup
        if epoch < warmup_epochs:
            lr = base_lr * (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

        for batch_X1, batch_X2, batch_y in dataloader:
            optimizer.zero_grad()
            pred = model(batch_X1, batch_X2)
            loss = criterion(pred, batch_y)

            # L1 regularization
            l1_loss = 0
            for param in model.parameters():
                l1_loss += torch.sum(torch.abs(param))
            loss += 1e-6 * l1_loss

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            epoch_loss += loss.item()

        # Validation
        model.eval()
        with torch.no_grad():
            test_pred = model(
                torch.tensor(X1_test, dtype=torch.float32),
                torch.tensor(X2_test, dtype=torch.float32)
            )
            test_loss = criterion(test_pred, torch.tensor(y_test, dtype=torch.float32).unsqueeze(1))

        if epoch >= warmup_epochs:
            scheduler.step()

        # Early stopping
        if test_loss < best_loss:
            best_loss = test_loss
            patience = 0
        else:
            patience += 1

        if patience >= patience_limit:
            print(f"Early stopping at epoch {epoch}")
            break

        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Train={epoch_loss/len(dataloader):.6f}, Test={test_loss:.6f}")

    return model

def train_enhanced_kan_models(data: pd.DataFrame, vix_data: pd.DataFrame = None,
                             lookback: int = 30, vol_window: int = 30):
    """Train enhanced KAN models with improved performance"""

    print("Creating enhanced volatility features...")

    # Create different feature sets with real VIX data
    base_data = create_enhanced_volatility_features(data.copy(), vix_data, lookback, vol_window)
    sma_data = create_enhanced_volatility_features(data.copy(), vix_data, lookback, vol_window, include_sma=True)
    vix_data_features = create_enhanced_volatility_features(data.copy(), vix_data, lookback, vol_window, include_vix=True)

    # Train/test split
    split_idx = int(0.8 * len(base_data['base_features']))

    results = {}

    # 1. Enhanced Pure KAN
    print("\n1. Training Enhanced KAN Model...")

    X = base_data['base_features']
    y = base_data['targets']

    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # Enhanced scaling
    scaler_X = RobustScaler()  # More robust to outliers
    scaler_y = StandardScaler()

    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

    # Train enhanced KAN
    kan_model = EnhancedVolatilityKAN(
        input_dim=X.shape[1],
        hidden_dims=[128, 64, 32],
        output_dim=1,
        dropout_rate=0.2,
        use_skip_connections=True
    )

    kan_optimizer = optim.AdamW(kan_model.parameters(), lr=0.001, weight_decay=1e-4)
    kan_scheduler = optim.lr_scheduler.OneCycleLR(
        kan_optimizer, max_lr=0.01, steps_per_epoch=50, epochs=200
    )

    # Training loop with improved techniques
    kan_model = train_model_with_advanced_techniques(
        kan_model, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled,
        kan_optimizer, kan_scheduler, epochs=200, batch_size=64
    )

    # Get predictions
    kan_model.eval()
    with torch.no_grad():
        kan_pred = kan_model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy()
        kan_pred = scaler_y.inverse_transform(kan_pred).flatten()

    results['enhanced_kan'] = {
        'model': kan_model,
        'predictions': kan_pred,
        'model_type': 'Enhanced KAN'
    }

    # 2. KAN + SMA Model
    print("\n2. Training KAN + SMA Model...")

    X_sma = sma_data['sma_features']
    X_sma_train, X_sma_test = X_sma[:split_idx], X_sma[split_idx:]

    scaler_sma = StandardScaler()
    X_sma_train_scaled = scaler_sma.fit_transform(X_sma_train)
    X_sma_test_scaled = scaler_sma.transform(X_sma_test)

    kan_sma_model = KAN_SMA_Model(
        base_features=X.shape[1],
        sma_features=X_sma.shape[1],
        hidden_dims=[96, 64, 32]
    )

    kan_sma_optimizer = optim.AdamW(kan_sma_model.parameters(), lr=0.001, weight_decay=1e-4)
    kan_sma_scheduler = optim.lr_scheduler.OneCycleLR(
        kan_sma_optimizer, max_lr=0.008, steps_per_epoch=50, epochs=200
    )

    # Train KAN+SMA
    kan_sma_model = train_dual_input_model(
        kan_sma_model, X_train_scaled, X_sma_train_scaled, y_train_scaled,
        X_test_scaled, X_sma_test_scaled, y_test_scaled,
        kan_sma_optimizer, kan_sma_scheduler, epochs=200
    )

    # Get predictions
    kan_sma_model.eval()
    with torch.no_grad():
        kan_sma_pred = kan_sma_model(
            torch.tensor(X_test_scaled, dtype=torch.float32),
            torch.tensor(X_sma_test_scaled, dtype=torch.float32)
        ).numpy()
        kan_sma_pred = scaler_y.inverse_transform(kan_sma_pred).flatten()

    results['kan_sma'] = {
        'model': kan_sma_model,
        'predictions': kan_sma_pred,
        'model_type': 'KAN + SMA'
    }

    # 3. KAN + VIX Model
    print("\n3. Training KAN + VIX Model...")

    X_vix = vix_data_features['vix_features']
    X_vix_train, X_vix_test = X_vix[:split_idx], X_vix[split_idx:]

    scaler_vix = StandardScaler()
    X_vix_train_scaled = scaler_vix.fit_transform(X_vix_train)
    X_vix_test_scaled = scaler_vix.transform(X_vix_test)

    kan_vix_model = KAN_VIX_Model(
        base_features=X.shape[1],
        vix_features=X_vix.shape[1],
        hidden_dims=[128, 64, 32]
    )

    kan_vix_optimizer = optim.AdamW(kan_vix_model.parameters(), lr=0.001, weight_decay=1e-4)
    kan_vix_scheduler = optim.lr_scheduler.OneCycleLR(
        kan_vix_optimizer, max_lr=0.008, steps_per_epoch=50, epochs=200
    )

    # Train KAN+VIX
    kan_vix_model = train_dual_input_model(
        kan_vix_model, X_train_scaled, X_vix_train_scaled, y_train_scaled,
        X_test_scaled, X_vix_test_scaled, y_test_scaled,
        kan_vix_optimizer, kan_vix_scheduler, epochs=200
    )

    # Get predictions
    kan_vix_model.eval()
    with torch.no_grad():
        kan_vix_pred = kan_vix_model(
            torch.tensor(X_test_scaled, dtype=torch.float32),
            torch.tensor(X_vix_test_scaled, dtype=torch.float32)
        ).numpy()
        kan_vix_pred = scaler_y.inverse_transform(kan_vix_pred).flatten()

    results['kan_vix'] = {
        'model': kan_vix_model,
        'predictions': kan_vix_pred,
        'model_type': 'KAN + VIX'
    }

    return results, y_test

def compare_all_models(data: pd.DataFrame, vix_data: pd.DataFrame = None,
                      lookback: int = 30, vol_window: int = 30):
    """Compare all enhanced models including MLP baseline"""

    print("=" * 80)
    print("ENHANCED MODEL COMPARISON: KAN vs MLP")
    print("=" * 80)

    # Train enhanced KAN models
    kan_results, y_test = train_enhanced_kan_models(data, vix_data, lookback, vol_window)

    # Train enhanced MLP for comparison
    print("\n4. Training Original MLP Baseline...")

    base_data = create_enhanced_volatility_features(data.copy(), vix_data, lookback, vol_window)
    X = base_data['base_features']
    y = base_data['targets']

    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test_mlp = y[:split_idx], y[split_idx:]

    scaler_X = StandardScaler()  # Use simple StandardScaler for original MLP
    scaler_y = StandardScaler()

    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test_mlp.reshape(-1, 1)).flatten()

    mlp_model = OriginalMLP(
        input_dim=X.shape[1],
        hidden_dims=[64, 32, 16],  # Same as original
        output_dim=1,
        dropout_rate=0.3
    )

    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.01)  # Simple Adam optimizer
    mlp_scheduler = optim.lr_scheduler.StepLR(mlp_optimizer, step_size=50, gamma=0.5)  # Simple scheduler

    # Simple training loop for original MLP
    criterion = nn.MSELoss()
    dataset = TensorDataset(
        torch.tensor(X_train_scaled, dtype=torch.float32),
        torch.tensor(y_train_scaled, dtype=torch.float32).unsqueeze(1)
    )
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    print("Training Original MLP...")
    for epoch in range(100):  # Fewer epochs
        mlp_model.train()
        for batch_X, batch_y in dataloader:
            mlp_optimizer.zero_grad()
            pred = mlp_model(batch_X)
            loss = criterion(pred, batch_y)
            loss.backward()
            mlp_optimizer.step()

        mlp_scheduler.step()

        if epoch % 20 == 0:
            mlp_model.eval()
            with torch.no_grad():
                test_pred = mlp_model(torch.tensor(X_test_scaled, dtype=torch.float32))
                test_loss = criterion(test_pred, torch.tensor(y_test_scaled, dtype=torch.float32).unsqueeze(1))
                print(f"MLP Epoch {epoch}: Test Loss = {test_loss:.6f}")

    # Get MLP predictions
    mlp_model.eval()
    with torch.no_grad():
        mlp_pred = mlp_model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy()
        mlp_pred = scaler_y.inverse_transform(mlp_pred).flatten()

    kan_results['original_mlp'] = {
        'model': mlp_model,
        'predictions': mlp_pred,
        'model_type': 'Original MLP'
    }

    # Evaluate all models
    print("\n" + "=" * 80)
    print("COMPREHENSIVE MODEL EVALUATION")
    print("=" * 80)

    print(f"{'Model':<20} {'RMSE':<12} {'MAE':<12} {'MAPE':<12} {'RÂ²':<8}")
    print("-" * 65)

    for model_name, model_data in kan_results.items():
        predictions = model_data['predictions']

        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)

        # MAPE with epsilon
        epsilon = 1e-8
        mape = np.mean(np.abs((y_test - predictions) / (np.abs(y_test) + epsilon))) * 100

        r2 = r2_score(y_test, predictions)

        model_type = model_data.get('model_type', model_name)
        print(f"{model_type:<20} {rmse:<12.6f} {mae:<12.6f} {mape:<12.2f} {r2:<8.4f}")

    # Find best performing model
    best_model = min(kan_results.items(),
                    key=lambda x: np.sqrt(mean_squared_error(y_test, x[1]['predictions'])))

    print(f"\nðŸ† Best Model: {best_model[1]['model_type']}")
    print(f"   RMSE: {np.sqrt(mean_squared_error(y_test, best_model[1]['predictions'])):.6f}")

    # Check if KAN models beat MLP
    kan_models = ['enhanced_kan', 'kan_sma', 'kan_vix']
    mlp_rmse = np.sqrt(mean_squared_error(y_test, kan_results['original_mlp']['predictions']))

    print(f"\nðŸ“Š KAN vs Original MLP Performance:")
    print(f"   Original MLP RMSE: {mlp_rmse:.6f}")

    for kan_model in kan_models:
        if kan_model in kan_results:
            kan_rmse = np.sqrt(mean_squared_error(y_test, kan_results[kan_model]['predictions']))
            improvement = ((mlp_rmse - kan_rmse) / mlp_rmse) * 100
            status = "âœ… Better" if kan_rmse < mlp_rmse else "âŒ Worse"
            print(f"   {kan_results[kan_model]['model_type']}: {kan_rmse:.6f} ({improvement:+.2f}%) {status}")

    return kan_results, y_test

def create_visualization(results, y_test):
    """Create comprehensive visualization of all models"""

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Enhanced KAN Models vs MLP - Volatility Predictions', fontsize=16, fontweight='bold')

    plot_models = ['enhanced_kan', 'kan_sma', 'kan_vix', 'original_mlp']
    plot_titles = ['Enhanced KAN', 'KAN + SMA', 'KAN + VIX', 'Original MLP']
    colors = ['blue', 'green', 'orange', 'red']

    for idx, (model_key, title, color) in enumerate(zip(plot_models, plot_titles, colors)):
        if model_key in results:
            row = idx // 2
            col = idx % 2

            predictions = results[model_key]['predictions']

            # Plot last 100 points for clarity
            plot_len = min(100, len(y_test))
            actual = y_test[-plot_len:]
            pred = predictions[-plot_len:]

            axes[row, col].plot(actual, label='Actual', color='black', linewidth=2)
            axes[row, col].plot(pred, label='Predicted', color=color, linestyle='--', linewidth=2, alpha=0.8)

            # Calculate and display RÂ²
            r2 = r2_score(actual, pred)
            rmse = np.sqrt(mean_squared_error(actual, pred))

            axes[row, col].set_title(f'{title}', fontsize=14, fontweight='bold')
            axes[row, col].set_xlabel('Time')
            axes[row, col].set_ylabel('Volatility')
            axes[row, col].legend()
            axes[row, col].grid(True, alpha=0.3)

            # Add metrics text
            axes[row, col].text(0.02, 0.98, f"RÂ² = {r2:.4f}\nRMSE = {rmse:.6f}",
                              transform=axes[row, col].transAxes, verticalalignment='top',
                              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

    plt.tight_layout()
    plt.show()

    # Create performance comparison bar chart
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    model_names = []
    rmse_values = []
    r2_values = []

    for model_key, model_data in results.items():
        predictions = model_data['predictions']
        model_names.append(model_data['model_type'])
        rmse_values.append(np.sqrt(mean_squared_error(y_test, predictions)))
        r2_values.append(r2_score(y_test, predictions))

    # RMSE comparison
    colors = ['blue', 'green', 'orange', 'red']
    ax1.bar(model_names, rmse_values, color=colors[:len(model_names)], alpha=0.7)
    ax1.set_title('Model RMSE Comparison', fontsize=14, fontweight='bold')
    ax1.set_ylabel('RMSE')
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(True, alpha=0.3)

    # RÂ² comparison
    ax2.bar(model_names, r2_values, color=colors[:len(model_names)], alpha=0.7)
    ax2.set_title('Model RÂ² Comparison', fontsize=14, fontweight='bold')
    ax2.set_ylabel('RÂ² Score')
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

def main_enhanced_comparison():
    """Main function to run enhanced KAN vs MLP comparison"""

    print("ðŸš€ Enhanced KAN Models vs Original MLP for Volatility Prediction")
    print("=" * 80)
    print("Using ORIGINAL B-spline KAN implementation with improvements:")
    print("âœ“ Learnable adaptive grids")
    print("âœ“ Skip connections and residual learning")
    print("âœ“ Multi-head attention for feature fusion")
    print("âœ“ Regime detection for VIX model")
    print("âœ“ Advanced training techniques")
    print("âœ“ Enhanced feature engineering")
    print("\nComparing against simple Original MLP baseline")
    print("=" * 80)

    # Load your data
    data = pd.read_csv('/content/drive/My Drive/fpt.csv', index_col=False)
    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]
    print(f"Loaded stock data: {data.shape}")
    print(f"Stock data columns: {list(data.columns)}")

    vix_data = pd.read_csv('/content/drive/My Drive/vix.csv', index_col=False)
    vix_data = vix_data.loc[:, ~vix_data.columns.str.contains('^Unnamed')]
    print(f"Loaded VIX data: {vix_data.shape}")
    print(f"VIX data columns: {list(vix_data.columns)}")

    # Run comparison
    results, y_test = compare_all_models(data, vix_data, lookback=30, vol_window=30)

    # Create comprehensive visualization
    create_visualization(results, y_test)

    # Additional analysis
    print("\n" + "=" * 80)
    print("ðŸ” DETAILED ANALYSIS")
    print("=" * 80)

    # Volatility regime analysis
    print("\nðŸ“ˆ Volatility Regime Performance:")

    # Define regimes based on percentiles
    low_threshold = np.percentile(y_test, 25)
    high_threshold = np.percentile(y_test, 75)

    low_vol_mask = y_test <= low_threshold
    normal_vol_mask = (y_test > low_threshold) & (y_test <= high_threshold)
    high_vol_mask = y_test > high_threshold

    regimes = {
        'Low Volatility': low_vol_mask,
        'Normal Volatility': normal_vol_mask,
        'High Volatility': high_vol_mask
    }

    print(f"{'Model':<20} {'Low Vol RÂ²':<12} {'Normal Vol RÂ²':<14} {'High Vol RÂ²':<12}")
    print("-" * 58)

    for model_name, model_data in results.items():
        predictions = model_data['predictions']
        model_type = model_data['model_type']

        regime_r2 = []
        for regime_name, mask in regimes.items():
            if np.sum(mask) > 0:
                r2 = r2_score(y_test[mask], predictions[mask])
                regime_r2.append(r2)
            else:
                regime_r2.append(0)

        print(f"{model_type:<20} {regime_r2[0]:<12.4f} {regime_r2[1]:<14.4f} {regime_r2[2]:<12.4f}")

    # Feature importance analysis (for interpretability)
    print(f"\nðŸ”¬ Model Complexity Analysis:")
    total_params = {}
    for model_name, model_data in results.items():
        model = model_data['model']
        total_params[model_data['model_type']] = sum(p.numel() for p in model.parameters())

    print(f"{'Model':<20} {'Parameters':<12} {'Params/Performance':<15}")
    print("-" * 47)
    for model_name, model_data in results.items():
        model_type = model_data['model_type']
        params = total_params[model_type]
        rmse = np.sqrt(mean_squared_error(y_test, model_data['predictions']))
        efficiency = params / (1/rmse)  # Lower is better
        print(f"{model_type:<20} {params:<12,} {efficiency:<15,.0f}")

    print("\nðŸŽ¯ Key Insights:")

    # Find best KAN model
    kan_models = ['enhanced_kan', 'kan_sma', 'kan_vix']
    best_kan = min([(k, v) for k, v in results.items() if k in kan_models],
                  key=lambda x: np.sqrt(mean_squared_error(y_test, x[1]['predictions'])))

    mlp_rmse = np.sqrt(mean_squared_error(y_test, results['original_mlp']['predictions']))
    best_kan_rmse = np.sqrt(mean_squared_error(y_test, best_kan[1]['predictions']))

    if best_kan_rmse < mlp_rmse:
        improvement = ((mlp_rmse - best_kan_rmse) / mlp_rmse) * 100
        print(f"âœ… Best KAN model ({best_kan[1]['model_type']}) outperforms Original MLP by {improvement:.2f}%")
        print(f"ðŸŽ‰ KAN success! B-splines + enhancements beat traditional neural networks")
    else:
        print(f"âŒ Original MLP still outperforms KAN models")
        print(f"ðŸ’¡ Consider: More data, different hyperparameters, or domain-specific features")

    print(f"\nðŸ Conclusion:")
    best_overall = min(results.items(), key=lambda x: np.sqrt(mean_squared_error(y_test, x[1]['predictions'])))
    print(f"   Best overall model: {best_overall[1]['model_type']}")
    print(f"   RMSE: {np.sqrt(mean_squared_error(y_test, best_overall[1]['predictions'])):.6f}")

    # B-spline specific insights
    print(f"\nðŸ”§ B-spline KAN Insights:")
    print(f"   â€¢ Using original B-spline implementation with learnable grids")
    print(f"   â€¢ Grid adaptation allows focusing on important input regions")
    print(f"   â€¢ Skip connections help with deeper architectures")
    print(f"   â€¢ Feature engineering crucial for KAN performance")

    return results

if __name__ == "__main__":
    print("Starting Enhanced KAN vs MLP Comparison...")
    print("Using Original B-spline KAN Implementation")
    results = main_enhanced_comparison()
    print("\nâœ… Analysis complete!")