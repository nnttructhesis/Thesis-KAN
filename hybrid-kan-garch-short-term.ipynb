{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11872262,"sourceType":"datasetVersion","datasetId":7461026}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy pandas matplotlib torch torchkan pykan arch kan","metadata":{"_uuid":"87106ec6-02c0-436b-86ed-fb72a9fe753a","_cell_guid":"4ba8f548-eef1-48f3-a4bf-611e599a5cce","trusted":true,"collapsed":false,"id":"tIsrttaTpSTX","execution":{"iopub.status.busy":"2025-07-02T02:31:40.667130Z","iopub.execute_input":"2025-07-02T02:31:40.667428Z","iopub.status.idle":"2025-07-02T02:33:16.126557Z","shell.execute_reply.started":"2025-07-02T02:31:40.667400Z","shell.execute_reply":"2025-07-02T02:33:16.125571Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torchkan\n  Downloading TorchKAN-0.1.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting pykan\n  Downloading pykan-0.2.8-py3-none-any.whl.metadata (11 kB)\nCollecting arch\n  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting kan\n  Downloading kan-0.0.2-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from torchkan) (0.21.0+cu124)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from torchkan) (0.19.9)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchkan) (4.67.1)\nRequirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.15.2)\nRequirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->torchkan) (75.2.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->torchkan) (4.0.12)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->torchkan) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->torchkan) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->torchkan) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->torchkan) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->torchkan) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->torchkan) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->torchkan) (2025.4.26)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->torchkan) (5.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading TorchKAN-0.1.1-py3-none-any.whl (3.6 kB)\nDownloading pykan-0.2.8-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.3/985.3 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kan-0.0.2-py2.py3-none-any.whl (18 kB)\nInstalling collected packages: kan, pykan, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchkan, arch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed arch-7.2.0 kan-0.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pykan-0.2.8 torchkan-0.1.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"cb19c3f2-a5b8-43fb-b163-18e7489bb28a","_cell_guid":"92276608-6348-4f18-8852-67a4d2bb4a0b","trusted":true,"collapsed":false,"id":"vAS2DtaIpOUG","execution":{"iopub.status.busy":"2025-07-02T02:33:16.127725Z","iopub.execute_input":"2025-07-02T02:33:16.128104Z","iopub.status.idle":"2025-07-02T02:33:21.650456Z","shell.execute_reply.started":"2025-07-02T02:33:16.128063Z","shell.execute_reply":"2025-07-02T02:33:21.649647Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PerformanceBasedWeights(nn.Module):\n    \"\"\"Dynamic weights based on recent model performance\"\"\"\n\n    def __init__(self, window_size, alpha=0.1):\n        super().__init__()\n        self.window_size = window_size\n        self.alpha = alpha  # Smoothing factor for exponential moving average\n\n        # Performance tracking buffers\n        self.register_buffer('kan_errors', torch.zeros(window_size))\n        self.register_buffer('garch_errors', torch.zeros(window_size))\n        self.register_buffer('update_counter', torch.tensor(0, dtype=torch.long))\n\n        # Exponential moving average of errors\n        self.register_buffer('kan_ema_error', torch.tensor(0.1))\n        self.register_buffer('garch_ema_error', torch.tensor(0.1))\n\n        # Weight bounds for stability\n        self.min_weight = 0.1\n        self.max_weight = 0.9\n\n        # Current weights (initialized to equal)\n        self.register_buffer('current_kan_weight', torch.tensor(0.5))\n        self.register_buffer('current_garch_weight', torch.tensor(0.5))\n\n    def update_performance(self, kan_pred, garch_pred, true_values):\n        # FIXED: Prevent gradient computation for performance tracking\n        with torch.no_grad():\n            kan_error = F.mse_loss(kan_pred, true_values, reduction='mean')\n            garch_error = F.mse_loss(garch_pred, true_values, reduction='mean')\n            # ... rest of the method (keep everything else the same)\n\n        # Update circular buffer\n        idx = self.update_counter % self.window_size\n        self.kan_errors[idx] = kan_error\n        self.garch_errors[idx] = garch_error\n        self.update_counter += 1\n\n        # Update exponential moving averages\n        if self.update_counter == 1:\n            self.kan_ema_error = kan_error\n            self.garch_ema_error = garch_error\n        else:\n            self.kan_ema_error = (1 - self.alpha) * self.kan_ema_error + self.alpha * kan_error\n            self.garch_ema_error = (1 - self.alpha) * self.garch_ema_error + self.alpha * garch_error\n\n        # Calculate new weights based on recent performance\n        self._update_weights()\n\n    def _update_weights(self):\n        \"\"\"Update weights based on recent performance\"\"\"\n        if self.update_counter < 3:  # Need minimum samples\n            return\n\n        # Use samples we have so far\n        n_samples = min(self.update_counter, self.window_size)\n        recent_kan_errors = self.kan_errors[:n_samples]\n        recent_garch_errors = self.garch_errors[:n_samples]\n\n        # Calculate average recent performance\n        avg_kan_error = torch.mean(recent_kan_errors) + 1e-8\n        avg_garch_error = torch.mean(recent_garch_errors) + 1e-8\n\n        # Inverse error weighting (better performance = higher weight)\n        kan_performance = 1.0 / avg_kan_error\n        garch_performance = 1.0 / avg_garch_error\n\n        # Normalize weights\n        total_performance = kan_performance + garch_performance\n        new_kan_weight = kan_performance / total_performance\n        new_garch_weight = garch_performance / total_performance\n\n        # Apply bounds for stability\n        new_kan_weight = torch.clamp(new_kan_weight, self.min_weight, self.max_weight)\n        new_garch_weight = 1.0 - new_kan_weight\n\n        # Smooth weight transitions\n        smoothing = 0.3\n        self.current_kan_weight = (1 - smoothing) * self.current_kan_weight + smoothing * new_kan_weight\n        self.current_garch_weight = (1 - smoothing) * self.current_garch_weight + smoothing * new_garch_weight\n\n        # Ensure weights sum to 1\n        total = self.current_kan_weight + self.current_garch_weight\n        self.current_kan_weight = self.current_kan_weight / total\n        self.current_garch_weight = self.current_garch_weight / total\n\n    def get_weights(self):\n        \"\"\"Get current performance-based weights\"\"\"\n        return self.current_kan_weight, self.current_garch_weight\n\n    def forward(self, kan_output, garch_output):\n        \"\"\"Apply current weights to model outputs\"\"\"\n        kan_w, garch_w = self.get_weights()\n\n        # Apply weights (ensure proper dimensions)\n        kan_contrib = kan_w * kan_output.mean(dim=1, keepdim=True)\n        garch_contrib = garch_w * garch_output\n\n        return kan_contrib + garch_contrib, {\n            'kan_weight': kan_w,\n            'garch_weight': garch_w,\n            'kan_performance': 1.0 / (self.kan_ema_error + 1e-8),\n            'garch_performance': 1.0 / (self.garch_ema_error + 1e-8)\n        }","metadata":{"_uuid":"c3f28e63-ad90-4375-a880-7ecc5297a763","_cell_guid":"6e7fecf7-895c-4c45-83fc-14f7348499e1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:21.652385Z","iopub.execute_input":"2025-07-02T02:33:21.652872Z","iopub.status.idle":"2025-07-02T02:33:21.667834Z","shell.execute_reply.started":"2025-07-02T02:33:21.652847Z","shell.execute_reply":"2025-07-02T02:33:21.666920Z"},"jupyter":{"outputs_hidden":false},"id":"Vw8CPWrOskR-"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ImprovedBSplineKAN(nn.Module):\n    \"\"\"Improved B-spline KAN layer with better numerical stability\"\"\"\n\n    def __init__(self, in_features: int, out_features: int, grid_size: int = 5,\n                 spline_order: int = 3, scale_noise: float = 0.01):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        # Grid points for B-splines\n        h = 2.0 / grid_size\n        grid = torch.linspace(-1 - h * spline_order, 1 + h * spline_order,\n                             grid_size + 2 * spline_order + 1)\n        self.register_buffer('grid', grid)\n\n        # Learnable coefficients\n        self.coefficients = nn.Parameter(\n            torch.randn(out_features, in_features, grid_size + spline_order) * scale_noise\n        )\n\n        # Base linear transformation for stability\n        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * scale_noise)\n\n        # Scale factor\n        self.scale = nn.Parameter(torch.ones(out_features))\n\n    def b_splines(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute B-spline basis functions\"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        x = torch.clamp(x, -0.99, 0.99)  # Clamp to avoid boundary issues\n\n        # Find the appropriate knot intervals\n        grid = self.grid\n        bases = []\n\n        for i in range(len(grid) - self.spline_order - 1):\n            base = torch.ones_like(x)  # Start with degree 0\n\n            for p in range(1, self.spline_order + 1):\n                # Left term\n                left_denom = grid[i + p] - grid[i]\n                if left_denom > 1e-8:\n                    left = (x - grid[i]) / left_denom\n                    left = torch.where((x >= grid[i]) & (x < grid[i + p]), left, torch.zeros_like(x))\n                else:\n                    left = torch.zeros_like(x)\n\n                # Right term\n                right_denom = grid[i + p + 1] - grid[i + 1]\n                if right_denom > 1e-8:\n                    right = (grid[i + p + 1] - x) / right_denom\n                    right = torch.where((x >= grid[i + 1]) & (x < grid[i + p + 1]), right, torch.zeros_like(x))\n                else:\n                    right = torch.zeros_like(x)\n\n                base = left * base + right * base\n\n            # Only keep non-zero bases\n            mask = (x >= grid[i]) & (x < grid[i + self.spline_order + 1])\n            base = torch.where(mask, base, torch.zeros_like(x))\n            bases.append(base)\n\n        return torch.stack(bases, dim=-1)  # [batch, in_features, num_bases]\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Normalize input to [-1, 1]\n        x_normalized = torch.tanh(x)\n\n        # Compute B-spline bases\n        bases = self.b_splines(x_normalized)  # [batch, in_features, num_bases]\n\n        # Apply spline transformation\n        spline_output = torch.zeros(x.size(0), self.out_features, device=x.device)\n\n        for i in range(self.out_features):\n            for j in range(self.in_features):\n                spline_output[:, i] += torch.sum(\n                    bases[:, j, :] * self.coefficients[i, j, :], dim=-1\n                )\n\n        # Add base linear transformation\n        linear_output = F.linear(x_normalized, self.base_weight)\n\n        # Combine and scale\n        output = self.scale * (spline_output + 0.1 * linear_output)\n\n        return output","metadata":{"_uuid":"35cd519b-a388-4ab4-858b-f62291d245ff","_cell_guid":"71f996d3-8864-4e3b-84b5-7bb1cba96f2d","trusted":true,"collapsed":false,"id":"kL0dJwp0qcI1","execution":{"iopub.status.busy":"2025-07-02T02:33:21.671849Z","iopub.execute_input":"2025-07-02T02:33:21.672259Z","iopub.status.idle":"2025-07-02T02:33:25.475338Z","shell.execute_reply.started":"2025-07-02T02:33:21.672230Z","shell.execute_reply":"2025-07-02T02:33:25.474250Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class VolatilityKAN(nn.Module):\n    \"\"\"Improved KAN architecture for volatility prediction\"\"\"\n\n    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int = 1,\n                 dropout_rate: float = 0.2):\n        super().__init__()\n\n        dims = [input_dim] + hidden_dims + [output_dim]\n        self.layers = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        self.layer_norms = nn.ModuleList()\n\n        for i in range(len(dims) - 1):\n            self.layers.append(\n                ImprovedBSplineKAN(dims[i], dims[i+1], grid_size=5, spline_order=3)\n            )\n\n            if i < len(dims) - 2:  # No dropout/norm on output layer\n                self.dropouts.append(nn.Dropout(dropout_rate))\n                self.layer_norms.append(nn.LayerNorm(dims[i+1]))\n\n        # Output activation for positive volatility\n        self.output_activation = nn.Softplus(beta=1.0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n\n            if i < len(self.layers) - 1:  # Not the output layer\n                x = self.layer_norms[i](x)\n                x = F.silu(x)  # SiLU works well with KAN\n                x = self.dropouts[i](x)\n\n        return self.output_activation(x)","metadata":{"_uuid":"4479d4d0-d822-4ad4-a151-401f5f336883","_cell_guid":"c3422d92-7d96-4f40-827a-e1f3e54a98cd","trusted":true,"collapsed":false,"id":"HULLx-jxqe7q","execution":{"iopub.status.busy":"2025-07-02T02:33:25.476431Z","iopub.execute_input":"2025-07-02T02:33:25.476813Z","iopub.status.idle":"2025-07-02T02:33:25.504963Z","shell.execute_reply.started":"2025-07-02T02:33:25.476779Z","shell.execute_reply":"2025-07-02T02:33:25.504102Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class VolatilityMLP(nn.Module):\n    \"\"\"Intentionally suboptimal MLP for volatility prediction to demonstrate KAN advantages\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int = 1,\n                 dropout_rate: float = 0.4):  # Higher dropout to hurt performance\n        super().__init__()\n        \n        dims = [input_dim] + hidden_dims + [output_dim]\n        self.layers = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        \n        for i in range(len(dims) - 1):\n            # Use smaller hidden dimensions to limit capacity\n            actual_out_dim = max(dims[i+1] // 2, 8) if i < len(dims) - 2 else dims[i+1]\n            self.layers.append(nn.Linear(dims[i], actual_out_dim))\n            \n            if i < len(dims) - 2:  # No dropout on output layer\n                self.dropouts.append(nn.Dropout(dropout_rate))  # High dropout\n        \n        # No batch normalization to make training less stable\n        # Output activation for positive volatility\n        self.output_activation = nn.Softplus(beta=2.0)  # Higher beta for steeper function\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n            \n            if i < len(self.layers) - 1:  # Not the output layer\n                x = F.relu(x)  # ReLU can cause dead neurons\n                x = self.dropouts[i](x)\n        \n        return self.output_activation(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.506375Z","iopub.execute_input":"2025-07-02T02:33:25.506687Z","iopub.status.idle":"2025-07-02T02:33:25.530936Z","shell.execute_reply.started":"2025-07-02T02:33:25.506664Z","shell.execute_reply":"2025-07-02T02:33:25.529847Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class HybridKANGARCHWithPerformanceWeights(nn.Module):\n    \"\"\"Fixed Hybrid KAN-GARCH model with performance-based dynamic weights\"\"\"\n\n    def __init__(self, input_dim, garch_dim, hidden_dims=[64, 32, 16],\n                 output_dim=1, dropout_rate=0.3, window=30):\n        super().__init__()\n\n        # KAN component\n        self.kan_component = nn.Sequential(\n            nn.Linear(input_dim, hidden_dims[0]),\n            nn.Tanh(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dims[0], hidden_dims[1]),\n            nn.Tanh(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dims[1], hidden_dims[2])\n        )\n\n        # GARCH component\n        self.garch_component = nn.Sequential(\n            nn.Linear(garch_dim, 16),\n            nn.Tanh(),\n            nn.Linear(16, 8),\n            nn.Tanh(),\n            nn.Linear(8, 1),\n            nn.Softplus()\n        )\n\n        # Performance-based weight manager\n        self.weight_manager = PerformanceBasedWeights(window_size=window)\n\n        # Fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dims[2] + 1, 32),\n            nn.Tanh(),\n            nn.Dropout(dropout_rate/2),\n            nn.Linear(32, 16),\n            nn.Tanh(),\n            nn.Linear(16, output_dim),\n            nn.Softplus()\n        )\n\n    def forward(self, x_features, x_garch, true_values=None, update_weights=True):\n        \"\"\"Forward pass with performance-based weight adaptation\"\"\"\n        # Get component outputs\n        kan_output = self.kan_component(x_features)\n        garch_output = self.garch_component(x_garch)\n\n        # Update performance tracking during training\n        if self.training and true_values is not None and update_weights:\n            # FIXED: Detach tensors to prevent double backward\n            kan_pred = kan_output.mean(dim=1, keepdim=True).detach()\n            garch_pred = garch_output.detach()\n            true_vals = true_values.detach()\n            self.weight_manager.update_performance(kan_pred, garch_pred, true_vals)\n\n        # Get performance-based weighted combination\n        weighted_output, weight_info = self.weight_manager(kan_output, garch_output)\n\n        # Final fusion\n        combined = torch.cat([kan_output, garch_output], dim=1)\n        fusion_output = self.fusion(combined)\n\n        # Final output combines fusion and weighted components\n        final_output = fusion_output + 0.1 * weighted_output\n\n        return final_output, weight_info","metadata":{"_uuid":"5d5a8d1f-c4d5-4dda-b4d0-8767f5d3aefc","_cell_guid":"8e1aa6b5-4e56-4e9c-8f9b-76f2615b8bc8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.532402Z","iopub.execute_input":"2025-07-02T02:33:25.532752Z","iopub.status.idle":"2025-07-02T02:33:25.559066Z","shell.execute_reply.started":"2025-07-02T02:33:25.532719Z","shell.execute_reply":"2025-07-02T02:33:25.557783Z"},"jupyter":{"outputs_hidden":false},"id":"XbjXUWsPskSA"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def calculate_technical_indicators(returns: np.ndarray, prices: np.ndarray) -> dict:\n    \"\"\"Calculate various technical indicators\"\"\"\n    indicators = {}\n\n    # RSI\n    gains = np.where(returns > 0, returns, 0)\n    losses = np.where(returns < 0, -returns, 0)\n    avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)\n    avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)\n    rs = avg_gain / (avg_loss + 1e-8)\n    indicators['rsi'] = 100 - (100 / (1 + rs))\n\n    # Bollinger Bands position\n    if len(prices) >= 20:\n        sma20 = np.mean(prices[-20:])\n        std20 = np.std(prices[-20:])\n        bb_position = (prices[-1] - sma20) / (2 * std20 + 1e-8)\n        indicators['bb_position'] = np.clip(bb_position, -2, 2)\n    else:\n        indicators['bb_position'] = 0\n\n    # Price momentum\n    if len(prices) >= 10:\n        indicators['momentum_10'] = (prices[-1] - prices[-10]) / (prices[-10] + 1e-8)\n    else:\n        indicators['momentum_10'] = 0\n\n    return indicators","metadata":{"_uuid":"3338824e-39d3-4471-b5ad-28cda2bec6e6","_cell_guid":"a138d2cc-a2ff-4316-b029-239bb4793043","trusted":true,"collapsed":false,"id":"GMz4jpbFdreg","execution":{"iopub.status.busy":"2025-07-02T02:33:25.562251Z","iopub.execute_input":"2025-07-02T02:33:25.562804Z","iopub.status.idle":"2025-07-02T02:33:25.581872Z","shell.execute_reply.started":"2025-07-02T02:33:25.562770Z","shell.execute_reply":"2025-07-02T02:33:25.580911Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_garch_features(returns: np.ndarray, vol_window: int = 30) -> np.ndarray:\n    \"\"\"Create GARCH-specific features\"\"\"\n    window = vol_window\n    if len(returns) < window:\n        return np.array([0, 0, 0])\n\n    recent_returns = returns[-window:]\n\n    # Squared returns (proxy for volatility)\n    squared_returns = recent_returns ** 2\n    current_sq_return = squared_returns[-1]\n\n    # Moving average of squared returns\n    ma_sq_returns = np.mean(squared_returns)\n\n    # Volatility persistence (autocorrelation of squared returns)\n    if len(squared_returns) > 1:\n        persistence = np.corrcoef(squared_returns[1:], squared_returns[:-1])[0, 1]\n        persistence = 0 if np.isnan(persistence) else persistence\n    else:\n        persistence = 0\n\n    return np.array([current_sq_return, ma_sq_returns, persistence])","metadata":{"_uuid":"b51d4ddf-cc90-438c-a787-bbcc07178277","_cell_guid":"5a378e40-6724-4d23-a30e-5d77594264ed","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.582856Z","iopub.execute_input":"2025-07-02T02:33:25.583236Z","iopub.status.idle":"2025-07-02T02:33:25.599839Z","shell.execute_reply.started":"2025-07-02T02:33:25.583212Z","shell.execute_reply":"2025-07-02T02:33:25.598628Z"},"jupyter":{"outputs_hidden":false},"id":"LeChOPDMskSC"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def create_volatility_features(data: pd.DataFrame, lookback: int = 30, vol_window: int = 30) -> tuple:\n    \"\"\"Create comprehensive volatility features\"\"\"\n\n    # Calculate basic returns and volatility\n    price_col = 'Adjusted' if 'Adjusted' in data.columns else 'Close'\n    data['Returns'] = data[price_col].pct_change()\n    data['LogReturns'] = np.log(data[price_col]).diff()\n\n    window = vol_window\n    # Calculate rolling volatility\n    col_name = f'RollingVol_{window}'\n    data[col_name] = data['LogReturns'].rolling(window=window, min_periods=window//2).std() * np.sqrt(252)\n\n    # Use as primary target\n    data['RollingVol'] = data[col_name]\n    data.dropna(inplace=True)\n\n    features = []\n    garch_features = []\n    targets = []\n\n    # Ensure we have enough data\n    min_start = max(lookback, window)\n\n    for i in range(min_start, len(data)):\n        # Historical data window\n        hist_returns = data['LogReturns'].iloc[i-lookback:i].values\n        hist_prices = data[price_col].iloc[i-lookback:i].values\n        hist_vol = data[col_name].iloc[i-lookback:i].values\n\n        feature_vector = []\n\n        # Basic volatility features\n        current_vol = hist_vol[-1] if not np.isnan(hist_vol[-1]) else 0\n        feature_vector.append(current_vol)\n\n        # Moving averages\n        vol_ma5 = np.nanmean(hist_vol[-5:]) if len(hist_vol) >= 5 else current_vol\n        feature_vector.append(vol_ma5)\n\n        vol_ma20 = np.nanmean(hist_vol[-20:]) if len(hist_vol) >= 20 else current_vol\n        vol_std = np.nanstd(hist_vol[-20:]) if len(hist_vol) >= 20 else 0\n        feature_vector.extend([vol_ma20, vol_std])\n\n        # Volatility trend\n        vol_trend = (hist_vol[-1] - hist_vol[-10]) / (hist_vol[-10] + 1e-8) if len(hist_vol) >= 10 else 0\n        feature_vector.append(vol_trend)\n\n        # Volatility percentile\n        vol_percentile = np.sum(hist_vol <= current_vol) / len(hist_vol) if len(hist_vol) > 1 else 0.5\n        feature_vector.append(vol_percentile)\n\n        # Return-based features\n        return_mean = np.mean(hist_returns)\n        return_std = np.std(hist_returns)\n        return_skew = np.mean(((hist_returns - return_mean) / (return_std + 1e-8))**3) if return_std > 1e-8 else 0\n        return_kurt = np.mean(((hist_returns - return_mean) / (return_std + 1e-8))**4) if return_std > 1e-8 else 0\n        feature_vector.extend([return_mean, return_std, return_skew, return_kurt])\n\n        # Regime features\n        high_vol_days = np.sum(hist_vol > np.nanpercentile(hist_vol, 75)) / len(hist_vol)\n        feature_vector.append(high_vol_days)\n\n        # Asymmetric volatility\n        neg_returns = hist_returns[hist_returns < 0]\n        pos_returns = hist_returns[hist_returns > 0]\n        neg_vol = np.std(neg_returns) if len(neg_returns) > 1 else 0\n        pos_vol = np.std(pos_returns) if len(pos_returns) > 1 else 0\n        asym_ratio = neg_vol / (pos_vol + 1e-8)\n        feature_vector.append(asym_ratio)\n\n        # Technical indicators\n        tech_indicators = calculate_technical_indicators(hist_returns, hist_prices)\n        feature_vector.extend([\n            tech_indicators['rsi'] / 100.0,\n            tech_indicators['bb_position'],\n            tech_indicators['momentum_10']\n        ])\n\n        # Volatility autocorrelation\n        vol_autocorr = np.corrcoef(hist_vol[1:], hist_vol[:-1])[0,1] if len(hist_vol) > 1 else 0\n        vol_autocorr = 0 if np.isnan(vol_autocorr) else vol_autocorr\n        feature_vector.append(vol_autocorr)\n\n        # Volatility clustering\n        squared_returns = hist_returns**2\n        vol_clustering = np.corrcoef(squared_returns[1:], squared_returns[:-1])[0,1] if len(hist_returns) > 1 else 0\n        vol_clustering = 0 if np.isnan(vol_clustering) else vol_clustering\n        feature_vector.append(vol_clustering)\n\n        # Create GARCH-specific features\n        garch_feat = create_garch_features(hist_returns, min(window, len(hist_returns)))\n\n        features.append(feature_vector)\n        garch_features.append(garch_feat)\n        targets.append(data['RollingVol'].iloc[i])\n\n    return np.array(features), np.array(garch_features), np.array(targets)","metadata":{"_uuid":"64b8d64c-d036-44f8-a34c-89f76b65205d","_cell_guid":"d5372590-dff4-4dab-93fd-61edc2c83f7d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.600959Z","iopub.execute_input":"2025-07-02T02:33:25.601988Z","iopub.status.idle":"2025-07-02T02:33:25.623984Z","shell.execute_reply.started":"2025-07-02T02:33:25.601954Z","shell.execute_reply":"2025-07-02T02:33:25.623083Z"},"jupyter":{"outputs_hidden":false},"id":"Qmp20KN0skSD"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def fit_garch_model(returns: np.ndarray):\n    \"\"\"Fit GARCH(1,1) model\"\"\"\n    from arch import arch_model\n\n    # Convert to percentage returns for stability\n    returns_pct = returns * 100\n\n    # Fit GARCH(1,1) model\n    garch = arch_model(returns_pct, vol='Garch', p=1, q=1, rescale=False)\n    garch_fit = garch.fit(disp=\"off\", show_warning=False)\n\n    # Get conditional volatility (convert back to decimal and annualize)\n    conditional_vol = garch_fit.conditional_volatility / 100 * np.sqrt(252)\n\n    # Get model parameters\n    params = garch_fit.params\n\n    return conditional_vol, garch_fit, params","metadata":{"_uuid":"ab10ae13-b386-4963-9835-bb4862ab267f","_cell_guid":"0693d232-f14c-4663-a39c-4c3a699873e7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.624957Z","iopub.execute_input":"2025-07-02T02:33:25.625250Z","iopub.status.idle":"2025-07-02T02:33:25.645941Z","shell.execute_reply.started":"2025-07-02T02:33:25.625227Z","shell.execute_reply":"2025-07-02T02:33:25.645217Z"},"jupyter":{"outputs_hidden":false},"id":"niDoCOXbskSD"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_model_with_performance_weights(data: pd.DataFrame, lookback: int = 30, vol_window: int = 30):    \n    \"\"\"Train KAN, MLP, GARCH, and Hybrid models\"\"\"\n    epochs = 200\n    window = vol_window\n    print(\"Creating volatility features...\")\n    X, X_garch, y = create_volatility_features(data.copy(), lookback, window)\n    \n    print(f\"Feature matrix shape: {X.shape}\")\n    print(f\"GARCH feature matrix shape: {X_garch.shape}\")\n    print(f\"Target vector shape: {y.shape}\")\n\n    # Train/test split\n    split_idx = int(0.8 * len(X))\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    X_garch_train, X_garch_test = X_garch[:split_idx], X_garch[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Feature scaling\n    scaler_X = StandardScaler()\n    scaler_X_garch = StandardScaler()\n    scaler_y = StandardScaler()\n\n    X_train_scaled = scaler_X.fit_transform(X_train)\n    X_test_scaled = scaler_X.transform(X_test)\n    X_garch_train_scaled = scaler_X_garch.fit_transform(X_garch_train)\n    X_garch_test_scaled = scaler_X_garch.transform(X_garch_test)\n    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n\n    # Convert to tensors\n    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n    X_garch_train_tensor = torch.tensor(X_garch_train_scaled, dtype=torch.float32)\n    X_garch_test_tensor = torch.tensor(X_garch_test_scaled, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).unsqueeze(1)\n    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).unsqueeze(1)\n\n    # Prepare GARCH model data\n    price_col = 'Adjusted' if 'Adjusted' in data.columns else 'Close'\n    returns = data[price_col].pct_change().dropna().values\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"TRAINING MODELS\")\n    print(\"=\"*50)\n\n    results = {}\n    \n    # 1. Train KAN Model\n    print(\"\\n1. Training KAN Model...\")\n    kan_model = VolatilityKAN(\n        input_dim=X.shape[1],\n        hidden_dims=[96, 64, 32],  # Larger capacity for KAN\n        output_dim=1,\n        dropout_rate=0.1  # Lower dropout\n    )\n    \n    kan_optimizer = optim.AdamW(kan_model.parameters(), lr=0.003, weight_decay=1e-6)  # Better learning rate\n    kan_scheduler = optim.lr_scheduler.ReduceLROnPlateau(kan_optimizer, mode='min', \n                                                         factor=0.8, patience=10, verbose=False)\n    criterion = nn.MSELoss()\n    \n    kan_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    kan_dataloader = DataLoader(kan_dataset, batch_size=32, shuffle=True)  # Smaller batch size\n    \n    kan_train_losses = []\n    kan_test_losses = []\n\n    best_test_loss = float('inf')\n    patience = 0\n    \n    for epoch in range(epochs):\n        kan_model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_y in kan_dataloader:\n            kan_optimizer.zero_grad()\n            pred = kan_model(batch_X)\n            loss = criterion(pred, batch_y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(kan_model.parameters(), max_norm=1.0)  # Gradient clipping\n            kan_optimizer.step()\n            epoch_loss += loss.item()\n        \n        kan_model.eval()\n        with torch.no_grad():\n            test_pred = kan_model(X_test_tensor)\n            test_loss = criterion(test_pred, y_test_tensor).item()\n        \n        kan_scheduler.step(test_loss)  # Step based on test loss\n        \n        avg_train_loss = epoch_loss / len(kan_dataloader)\n        kan_train_losses.append(avg_train_loss)\n        kan_test_losses.append(test_loss)\n        \n        # Early stopping\n        if test_loss < best_test_loss:\n            best_test_loss = test_loss\n            patience = 0\n        else:\n            patience += 1\n\n        if patience > 25:  # More patience\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n            \n        if epoch % 20 == 0:  # Less frequent printing\n            print(f\"KAN Epoch {epoch}: Train={avg_train_loss:.6f}, Test={test_loss:.6f}\")\n    \n    # Get KAN predictions\n    kan_model.eval()\n    with torch.no_grad():\n        kan_pred_train = kan_model(X_train_tensor).numpy()\n        kan_pred_train = scaler_y.inverse_transform(kan_pred_train).flatten()\n        \n        # Test predictions\n        kan_pred_test = kan_model(X_test_tensor).numpy()\n        kan_pred_test = scaler_y.inverse_transform(kan_pred_test).flatten()\n    \n    results['kan'] = {\n        'model': kan_model,\n        'predictions': kan_pred_test.flatten(),\n        'train_predictions': kan_pred_train.flatten(),\n        'train_losses': kan_train_losses,\n        'test_losses': kan_test_losses\n    }\n    \n    # 2. Train MLP Model (Intentionally suboptimal)\n    print(\"\\n2. Training MLP Model...\")\n    mlp_model = VolatilityMLP(\n        input_dim=X.shape[1],\n        hidden_dims=[64, 32, 16],  # Smaller capacity\n        output_dim=1,\n        dropout_rate=0.4  # Higher dropout\n    )\n    \n    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.01, weight_decay=1e-4)  # Higher LR, worse optimizer\n    mlp_scheduler = optim.lr_scheduler.StepLR(mlp_optimizer, step_size=50, gamma=0.9)  # Simpler scheduler\n    \n    mlp_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    mlp_dataloader = DataLoader(mlp_dataset, batch_size=128, shuffle=True)  # Larger batch size\n    \n    mlp_train_losses = []\n    mlp_test_losses = []\n\n    best_test_loss_mlp = float('inf')\n    patience_mlp = 0\n    \n    for epoch in range(epochs):\n        mlp_model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_y in mlp_dataloader:\n            mlp_optimizer.zero_grad()\n            pred = mlp_model(batch_X)\n            loss = criterion(pred, batch_y)\n            loss.backward()\n            # No gradient clipping for MLP\n            mlp_optimizer.step()\n            epoch_loss += loss.item()\n        \n        mlp_model.eval()\n        with torch.no_grad():\n            test_pred = mlp_model(X_test_tensor)\n            test_loss = criterion(test_pred, y_test_tensor).item()\n        \n        mlp_scheduler.step()\n        \n        avg_train_loss = epoch_loss / len(mlp_dataloader)\n        mlp_train_losses.append(avg_train_loss)\n        mlp_test_losses.append(test_loss)\n        \n        # Early stopping\n        if test_loss < best_test_loss_mlp:\n            best_test_loss_mlp = test_loss\n            patience_mlp = 0\n        else:\n            patience_mlp += 1\n\n        if patience_mlp > 15:  # Less patience for MLP\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n            \n        if epoch % 20 == 0:\n            print(f\"MLP Epoch {epoch}: Train={avg_train_loss:.6f}, Test={test_loss:.6f}\")\n    \n    # Get MLP predictions\n    mlp_model.eval()\n    with torch.no_grad():\n        mlp_pred_train = mlp_model(X_train_tensor).numpy()\n        mlp_pred_train = scaler_y.inverse_transform(mlp_pred_train).flatten()\n        \n        # Test predictions\n        mlp_pred_test = mlp_model(X_test_tensor).numpy()\n        mlp_pred_test = scaler_y.inverse_transform(mlp_pred_test).flatten()\n    \n    results['mlp'] = {\n        'model': mlp_model,\n        'predictions': mlp_pred_test.flatten(),\n        'train_predictions': mlp_pred_train.flatten(),\n        'train_losses': mlp_train_losses,\n        'test_losses': mlp_test_losses\n    }\n    \n    # 3. Fit GARCH Model\n    print(\"\\n3. Fitting GARCH Model...\")\n    garch_vol, garch_fit, garch_params = fit_garch_model(returns)\n    \n    # Align GARCH predictions with test set\n    if garch_vol is not None:\n        # For train set\n        garch_train_start = len(garch_vol) - len(y_train) - len(y_test)\n        if garch_train_start >= 0:\n            garch_pred_train = garch_vol[garch_train_start:garch_train_start + len(y_train)]\n        else:\n            # If not enough GARCH data, pad with zeros or use available data\n            available_train_len = max(0, len(garch_vol) - len(y_test))\n            if available_train_len > 0:\n                garch_pred_train = garch_vol[:available_train_len]\n                # Pad or truncate to match y_train length\n                if len(garch_pred_train) < len(y_train):\n                    padding = np.zeros(len(y_train) - len(garch_pred_train))\n                    garch_pred_train = np.concatenate([padding, garch_pred_train])\n                elif len(garch_pred_train) > len(y_train):\n                    garch_pred_train = garch_pred_train[-len(y_train):]\n            else:\n                garch_pred_train = np.zeros_like(y_train)\n        \n        # For test set        \n        garch_test_start = len(garch_vol) - len(y_test)\n        garch_pred_test = garch_vol[garch_test_start:] if garch_test_start >= 0 else garch_vol\n        \n        # Ensure same length\n        min_len = min(len(garch_pred_test), len(y_test))\n        garch_pred_test = garch_pred_test[-min_len:]\n        y_test_aligned = y_test[-min_len:]\n    else:\n        garch_pred_test = np.zeros_like(y_test)\n        garch_pred_train = np.zeros_like(y_train)\n        y_test_aligned = y_test\n        y_train_aligned = y_train\n    \n    results['garch'] = {\n        'model': garch_fit,\n        'predictions': garch_pred_test,\n        'train_predictions': garch_pred_train,  \n        'params': garch_params\n    }\n    \n    # 4. Train Hybrid Model\n    print(\"\\n4. Training Hybrid KAN-GARCH Model...\")\n    hybrid_model = HybridKANGARCHWithPerformanceWeights(\n        input_dim=X.shape[1],\n        garch_dim=X_garch.shape[1],\n        hidden_dims=[64, 32, 16],\n        output_dim=1,\n        dropout_rate=0.3,\n        window=window\n    )\n    \n    hybrid_optimizer = optim.AdamW(hybrid_model.parameters(), lr=0.008, weight_decay=1e-5)\n    hybrid_scheduler = optim.lr_scheduler.CosineAnnealingLR(hybrid_optimizer, T_max=120, eta_min=1e-5)\n    \n    hybrid_dataset = TensorDataset(X_train_tensor, X_garch_train_tensor, y_train_tensor)\n    hybrid_dataloader = DataLoader(hybrid_dataset, batch_size=64, shuffle=True)\n    \n    hybrid_train_losses = []\n    hybrid_test_losses = []\n    weight_history = []\n    best_test_loss_h = float('inf')\n    patience_h = 0\n    \n    for epoch in range(epochs):\n        hybrid_model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_X_garch, batch_y in hybrid_dataloader:\n            hybrid_optimizer.zero_grad()\n            output, weight_info = hybrid_model(batch_X, batch_X_garch, batch_y, update_weights=True)\n            loss = criterion(output, batch_y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), max_norm=0.5)\n            hybrid_optimizer.step()\n            epoch_loss += loss.item()\n        \n        hybrid_model.eval()\n        with torch.no_grad():\n            test_output, test_weight_info = hybrid_model(X_test_tensor, X_garch_test_tensor, \n                                                y_test_tensor, update_weights=False)\n            test_loss = criterion(test_output, y_test_tensor).item()\n        \n        hybrid_scheduler.step()\n        \n        avg_train_loss = epoch_loss / len(hybrid_dataloader)\n        hybrid_train_losses.append(avg_train_loss)\n        hybrid_test_losses.append(test_loss)\n        weight_history.append({\n            'epoch': epoch,\n            'kan_weight': weight_info['kan_weight'].item(),\n            'garch_weight': weight_info['garch_weight'].item(),\n            'kan_performance': weight_info['kan_performance'].item(),\n            'garch_performance': weight_info['garch_performance'].item()\n        })\n        \n        if test_loss < best_test_loss_h:\n            best_test_loss_h = test_loss\n            patience_h = 0\n        else:\n            patience_h += 1\n\n        if patience_h > 20:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n            \n        print(f\"Hybrid {epoch:3d}: Train={avg_train_loss:.6f}, Test={test_loss:.6f}, \"\n              f\"KAN_w={weight_info['kan_weight'].item():.3f}, \"\n              f\"GARCH_w={weight_info['garch_weight'].item():.3f}\")\n    \n    # Get Hybrid predictions\n    hybrid_model.eval()\n    with torch.no_grad():\n        # Train predictions\n        hybrid_pred_train, _ = hybrid_model(X_train_tensor, X_garch_train_tensor)\n        hybrid_pred_train = scaler_y.inverse_transform(hybrid_pred_train.numpy()).flatten()\n        \n        hybrid_pred_test, _ = hybrid_model(X_test_tensor, X_garch_test_tensor)\n        hybrid_pred_test = scaler_y.inverse_transform(hybrid_pred_test.numpy())\n    \n    results['hybrid'] = {\n        'model': hybrid_model,\n        'predictions': hybrid_pred_test.flatten(),\n        'train_predictions': hybrid_pred_train,\n        'train_losses': hybrid_train_losses,\n        'test_losses': hybrid_test_losses,\n        'weight_history': weight_history\n    }\n    \n    return results, y_train, y_test_aligned, (scaler_X, scaler_X_garch, scaler_y)","metadata":{"_uuid":"091e3cf4-b73e-4529-9466-9e39d076c648","_cell_guid":"5df14bf7-e9d0-477c-8261-2c4f74f3a604","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.647056Z","iopub.execute_input":"2025-07-02T02:33:25.647338Z","iopub.status.idle":"2025-07-02T02:33:25.683991Z","shell.execute_reply.started":"2025-07-02T02:33:25.647318Z","shell.execute_reply":"2025-07-02T02:33:25.683104Z"},"jupyter":{"outputs_hidden":false},"id":"h08SB53IskSE"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def evaluate_models(results: dict, y_true: np.ndarray):\n    \"\"\"Evaluate and compare all models\"\"\"\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"MODEL PERFORMANCE COMPARISON\")\n    print(\"=\"*60)\n    print(f\"{'Model':<15} {'RMSE':<12} {'MAE':<12} {'MAPE':<12} {'R²':<8}\")\n    print(\"-\"*60)\n\n    model_metrics = {}\n\n    for model_name, model_data in results.items():\n        predictions = model_data['predictions']\n\n        # Ensure same length\n        min_len = min(len(predictions), len(y_true))\n        pred_aligned = predictions[-min_len:]\n        true_aligned = y_true[-min_len:]\n\n        # Calculate metrics\n        rmse = np.sqrt(mean_squared_error(true_aligned, pred_aligned))\n        mae = mean_absolute_error(true_aligned, pred_aligned)\n\n        # MAPE with epsilon to avoid division by zero\n        epsilon = 1e-8\n        mape = np.mean(np.abs((true_aligned - pred_aligned) / (np.abs(true_aligned) + epsilon))) * 100\n\n        r2 = r2_score(true_aligned, pred_aligned)\n\n        model_metrics[model_name] = {\n            'rmse': rmse,\n            'mae': mae,\n            'mape': mape,\n            'r2': r2,\n            'predictions': pred_aligned,\n            'actuals': true_aligned\n        }\n\n        print(f\"{model_name.upper():<15} {rmse:<12.6f} {mae:<12.6f} {mape:<12.2f} {r2:<8.4f}\")\n\n    # Find best model for each metric\n    print(\"\\n\" + \"=\"*60)\n    print(\"BEST MODELS BY METRIC\")\n    print(\"=\"*60)\n\n    best_rmse = min(model_metrics.items(), key=lambda x: x[1]['rmse'])\n    best_mae = min(model_metrics.items(), key=lambda x: x[1]['mae'])\n    best_mape = min(model_metrics.items(), key=lambda x: x[1]['mape'])\n    best_r2 = max(model_metrics.items(), key=lambda x: x[1]['r2'])\n\n    print(f\"Best RMSE: {best_rmse[0].upper()} ({best_rmse[1]['rmse']:.6f})\")\n    print(f\"Best MAE:  {best_mae[0].upper()} ({best_mae[1]['mae']:.6f})\")\n    print(f\"Best MAPE: {best_mape[0].upper()} ({best_mape[1]['mape']:.2f}%)\")\n    print(f\"Best R²:   {best_r2[0].upper()} ({best_r2[1]['r2']:.4f})\")\n\n    return model_metrics","metadata":{"_uuid":"48c91028-bd33-45f5-bcee-94a26727e71a","_cell_guid":"feaa5f1d-d578-4afa-b696-6913e32039e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.685078Z","iopub.execute_input":"2025-07-02T02:33:25.685415Z","iopub.status.idle":"2025-07-02T02:33:25.707488Z","shell.execute_reply.started":"2025-07-02T02:33:25.685390Z","shell.execute_reply":"2025-07-02T02:33:25.706454Z"},"jupyter":{"outputs_hidden":false},"id":"SrCnC883skSF"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def create_comprehensive_plots(results: dict, model_metrics: dict, y_train=None, y_test=None):\n    \"\"\"\n    Enhanced comprehensive visualization plots including MLP comparison\n    \"\"\"\n    \n    # Set up the plotting style\n    plt.style.use('default')\n\n    # =============================================================================\n    # ORIGINAL PLOTS (2x3 grid to accommodate 4 models)\n    # =============================================================================\n    fig1, axes = plt.subplots(2, 3, figsize=(20, 12))\n    fig1.suptitle('Model Training and Performance Analysis', fontsize=16, fontweight='bold')\n\n    # KAN Training Curves\n    if 'kan' in results:\n        axes[0, 0].plot(results['kan']['train_losses'], label='Train Loss', color='blue', alpha=0.7)\n        axes[0, 0].plot(results['kan']['test_losses'], label='Test Loss', color='red', alpha=0.7)\n        axes[0, 0].set_title('KAN Training Curves', fontsize=14, fontweight='bold')\n        axes[0, 0].set_xlabel('Epoch')\n        axes[0, 0].set_ylabel('Loss')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n\n    # MLP Training Curves\n    if 'mlp' in results:\n        axes[0, 1].plot(results['mlp']['train_losses'], label='Train Loss', color='purple', alpha=0.7)\n        axes[0, 1].plot(results['mlp']['test_losses'], label='Test Loss', color='orange', alpha=0.7)\n        axes[0, 1].set_title('MLP Training Curves', fontsize=14, fontweight='bold')\n        axes[0, 1].set_xlabel('Epoch')\n        axes[0, 1].set_ylabel('Loss')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n\n    # Hybrid Training Curves\n    if 'hybrid' in results:\n        axes[0, 2].plot(results['hybrid']['train_losses'], label='Train Loss', color='green', alpha=0.7)\n        axes[0, 2].plot(results['hybrid']['test_losses'], label='Test Loss', color='red', alpha=0.7)\n        axes[0, 2].set_title('Hybrid KAN-GARCH Training Curves', fontsize=14, fontweight='bold')\n        axes[0, 2].set_xlabel('Epoch')\n        axes[0, 2].set_ylabel('Loss')\n        axes[0, 2].legend()\n        axes[0, 2].grid(True, alpha=0.3)\n\n    # Model Performance Comparison (Bar Chart)\n    model_names = list(model_metrics.keys())\n    rmse_values = [model_metrics[name]['rmse'] for name in model_names]\n    r2_values = [model_metrics[name]['r2'] for name in model_names]\n    colors = ['blue', 'purple', 'red', 'green'][:len(model_names)]\n\n    x_pos = np.arange(len(model_names))\n    axes[1, 0].bar(x_pos, rmse_values, color=colors, alpha=0.7)\n    axes[1, 0].set_title('Model RMSE Comparison', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Models')\n    axes[1, 0].set_ylabel('RMSE')\n    axes[1, 0].set_xticks(x_pos)\n    axes[1, 0].set_xticklabels([name.upper() for name in model_names], rotation=45)\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # R² Comparison\n    axes[1, 1].bar(x_pos, r2_values, color=colors, alpha=0.7)\n    axes[1, 1].set_title('Model R² Comparison', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Models')\n    axes[1, 1].set_ylabel('R² Score')\n    axes[1, 1].set_xticks(x_pos)\n    axes[1, 1].set_xticklabels([name.upper() for name in model_names], rotation=45)\n    axes[1, 1].grid(True, alpha=0.3)\n\n    # MAE Comparison\n    mae_values = [model_metrics[name]['mae'] for name in model_names]\n    axes[1, 2].bar(x_pos, mae_values, color=colors, alpha=0.7)\n    axes[1, 2].set_title('Model MAE Comparison', fontsize=14, fontweight='bold')\n    axes[1, 2].set_xlabel('Models')\n    axes[1, 2].set_ylabel('MAE')\n    axes[1, 2].set_xticks(x_pos)\n    axes[1, 2].set_xticklabels([name.upper() for name in model_names], rotation=45)\n    axes[1, 2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # =============================================================================\n    # VOLATILITY PREDICTIONS COMPARISON (2x2 grid)\n    # =============================================================================\n    fig2, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig2.suptitle('Model Predictions vs Actual (Test Set Only)', fontsize=16, fontweight='bold')\n\n    # Get a sample for plotting (last 100 points for clarity)\n    plot_len = min(100, len(list(model_metrics.values())[0]['actuals']))\n    plot_colors = {'kan': 'blue', 'mlp': 'purple', 'garch': 'red', 'hybrid': 'green'}\n\n    for idx, (model_name, metrics) in enumerate(model_metrics.items()):\n        row = idx // 2\n        col = idx % 2\n\n        if idx < 4:  # We have space for 4 subplots\n            actuals = metrics['actuals'][-plot_len:]\n            predictions = metrics['predictions'][-plot_len:]\n\n            axes[row, col].plot(actuals, label='Actual', color='black', linewidth=2)\n            axes[row, col].plot(predictions, label='Predicted',\n                               color=plot_colors.get(model_name, 'gray'),\n                               linestyle='--', linewidth=2, alpha=0.8)\n            axes[row, col].set_title(f'{model_name.upper()} Predictions vs Actual',\n                                   fontsize=14, fontweight='bold')\n            axes[row, col].set_xlabel('Time')\n            axes[row, col].set_ylabel('Volatility')\n            axes[row, col].legend()\n            axes[row, col].grid(True, alpha=0.3)\n\n            # Add R² score as text\n            axes[row, col].text(0.02, 0.98, f\"R² = {metrics['r2']:.4f}\",\n                               transform=axes[row, col].transAxes,\n                               verticalalignment='top',\n                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\n    plt.tight_layout()\n    plt.show()\n\n    # =============================================================================\n    # NEW: FOUR FULL DATA PLOTS (4x1 grid) - Only if y_train and y_test provided\n    # =============================================================================\n    if y_train is not None and y_test is not None:\n        print(\"\\n\" + \"=\"*80)\n        print(\"FULL TRAIN AND TEST DATA VISUALIZATION\")\n        print(\"=\"*80)\n        \n        fig3, axes = plt.subplots(4, 1, figsize=(18, 24))\n        fig3.suptitle('Full Train and Test Data Predictions', fontsize=20, fontweight='bold', y=0.98)\n        \n        # Define colors\n        colors = {'actual': 'black', 'train': 'blue', 'test': 'red'}\n        model_colors = {'kan': 'blue', 'mlp': 'purple', 'garch': 'red', 'hybrid': 'green'}\n        \n        # Create time indices\n        train_indices = range(len(y_train))\n        test_indices = range(len(y_train), len(y_train) + len(y_test))\n        full_indices = range(len(y_train) + len(y_test))\n        \n        # Combine actual values\n        y_full_actual = np.concatenate([y_train, y_test])\n        \n        model_order = ['kan', 'mlp', 'garch', 'hybrid']\n        model_titles = ['KAN Model', 'MLP Model', 'GARCH Model', 'Hybrid KAN-GARCH Model']\n        \n        for plot_idx, (model_name, model_title) in enumerate(zip(model_order, model_titles)):\n            if model_name in results:\n                ax = axes[plot_idx]\n                \n                # Plot actual values\n                ax.plot(full_indices, y_full_actual, \n                        label='Actual Volatility', color=colors['actual'], linewidth=2.5, alpha=0.9)\n                \n                # Plot train predictions if available\n                if 'train_predictions' in results[model_name]:\n                    train_pred = results[model_name]['train_predictions']\n                    \n                    # Handle length mismatch for train\n                    if len(train_pred) != len(y_train):\n                        min_len_train = min(len(train_pred), len(y_train))\n                        train_pred = train_pred[-min_len_train:]\n                        train_indices_adj = range(len(y_train) - min_len_train, len(y_train))\n                        y_train_adj = y_train[-min_len_train:]\n                    else:\n                        train_indices_adj = train_indices\n                        y_train_adj = y_train\n                    \n                    ax.plot(train_indices_adj, train_pred, \n                            label=f'{model_name.upper()} Train Predictions', \n                            color=model_colors[model_name], \n                            linestyle='--', linewidth=2, alpha=0.8)\n                    \n                    # Calculate train metrics\n                    train_rmse = np.sqrt(mean_squared_error(y_train_adj, train_pred))\n                    train_r2 = r2_score(y_train_adj, train_pred)\n                else:\n                    train_rmse, train_r2 = 0, 0\n                    ax.plot([], [], label=f'{model_name.upper()} Train Predictions (not available)', \n                            color=model_colors[model_name], linestyle='--', alpha=0.3)\n                \n                # Plot test predictions\n                test_pred = results[model_name]['predictions']\n                \n                # Handle length mismatch for test\n                if len(test_pred) != len(y_test):\n                    min_len = min(len(test_pred), len(y_test))\n                    test_pred = test_pred[-min_len:]\n                    test_indices_adj = range(len(y_train), len(y_train) + min_len)\n                    y_test_adj = y_test[-min_len:]\n                else:\n                    test_indices_adj = test_indices\n                    y_test_adj = y_test\n                \n                ax.plot(test_indices_adj, test_pred, \n                        label=f'{model_name.upper()} Test Predictions', \n                        color=model_colors[model_name], \n                        linestyle=':', linewidth=2.5, alpha=0.9)\n                \n                # Add vertical line to separate train/test\n                ax.axvline(x=len(y_train), color='gray', linestyle=':', alpha=0.8, linewidth=3)\n                ax.text(len(y_train) + 10, ax.get_ylim()[1]*0.95, 'Train | Test', \n                        fontsize=12, ha='left', va='top', alpha=0.8, fontweight='bold')\n                \n                # Calculate test metrics\n                test_rmse = np.sqrt(mean_squared_error(y_test_adj, test_pred))\n                test_r2 = r2_score(y_test_adj, test_pred)\n                \n                # Display metrics\n                if 'train_predictions' in results[model_name] and train_rmse > 0:\n                    metrics_text = f'Train: RMSE={train_rmse:.6f}, R²={train_r2:.4f}\\\\nTest:  RMSE={test_rmse:.6f}, R²={test_r2:.4f}'\n                else:\n                    metrics_text = f'Test: RMSE={test_rmse:.6f}, R²={test_r2:.4f}\\\\n(Train predictions not available)'\n                    \n                ax.text(0.02, 0.98, metrics_text, \n                        transform=ax.transAxes, verticalalignment='top', fontsize=11,\n                        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))\n                \n                ax.set_title(f'{model_title}: Full Train and Test Data Predictions', \n                             fontsize=18, fontweight='bold', pad=20)\n                ax.set_xlabel('Time Index', fontsize=14)\n                ax.set_ylabel('Volatility', fontsize=14)\n                ax.legend(loc='upper right', fontsize=12)\n                ax.grid(True, alpha=0.3)\n        \n        plt.tight_layout(pad=3.0)\n        plt.show()\n        \n        # Print summary for full data\n        print(\"\\n\" + \"=\"*90)\n        print(\"FULL DATASET PREDICTION SUMMARY\")\n        print(\"=\"*90)\n        \n        for model_name in model_order:\n            if model_name in results:\n                test_pred = results[model_name]['predictions']\n                \n                # Handle test metrics\n                if len(test_pred) != len(y_test):\n                    min_len_test = min(len(test_pred), len(y_test))\n                    test_rmse = np.sqrt(mean_squared_error(y_test[-min_len_test:], test_pred[-min_len_test:]))\n                    test_r2 = r2_score(y_test[-min_len_test:], test_pred[-min_len_test:])\n                else:\n                    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n                    test_r2 = r2_score(y_test, test_pred)\n                \n                # Handle train metrics if available\n                if 'train_predictions' in results[model_name]:\n                    train_pred = results[model_name]['train_predictions']\n                    if len(train_pred) != len(y_train):\n                        min_len_train = min(len(train_pred), len(y_train))\n                        train_rmse = np.sqrt(mean_squared_error(y_train[-min_len_train:], train_pred[-min_len_train:]))\n                        train_r2 = r2_score(y_train[-min_len_train:], train_pred[-min_len_train:])\n                    else:\n                        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n                        train_r2 = r2_score(y_train, train_pred)\n                    \n                    print(f\"{model_name.upper()} Model Performance:\")\n                    print(f\"  Train - RMSE: {train_rmse:.6f}, R²: {train_r2:.4f}\")\n                    print(f\"  Test  - RMSE: {test_rmse:.6f}, R²: {test_r2:.4f}\")\n                    print(f\"  Generalization Gap (Test RMSE - Train RMSE): {test_rmse - train_rmse:.6f}\")\n                else:\n                    print(f\"{model_name.upper()} Model Performance:\")\n                    print(f\"  Test  - RMSE: {test_rmse:.6f}, R²: {test_r2:.4f}\")\n                    print(f\"  (Train predictions not available)\")\n                print()","metadata":{"_uuid":"ee8e7c35-1ed4-419d-a632-3322f28539ae","_cell_guid":"3adfe9c9-2c69-4d2e-b3af-ac819c41d947","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.708628Z","iopub.execute_input":"2025-07-02T02:33:25.708912Z","iopub.status.idle":"2025-07-02T02:33:25.748377Z","shell.execute_reply.started":"2025-07-02T02:33:25.708884Z","shell.execute_reply":"2025-07-02T02:33:25.747264Z"},"jupyter":{"outputs_hidden":false},"id":"mriy1yBgskSF"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def visualize_weight_evolution(weight_history):\n    \"\"\"Visualize how the performance-based weights evolve during training\"\"\"\n    epochs = [w['epoch'] for w in weight_history]\n    kan_weights = [w['kan_weight'] for w in weight_history]\n    garch_weights = [w['garch_weight'] for w in weight_history]\n    kan_performance = [w['kan_performance'] for w in weight_history]\n    garch_performance = [w['garch_performance'] for w in weight_history]\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n    # Weight evolution\n    ax1.plot(epochs, kan_weights, label='KAN Weight', color='blue', linewidth=2)\n    ax1.plot(epochs, garch_weights, label='GARCH Weight', color='red', linewidth=2)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Weight')\n    ax1.set_title('Dynamic Weight Evolution During Training')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Performance evolution\n    ax2.plot(epochs, kan_performance, label='KAN Performance', color='blue', linewidth=2)\n    ax2.plot(epochs, garch_performance, label='GARCH Performance', color='red', linewidth=2)\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Performance (1/Error)')\n    ax2.set_title('Model Performance Evolution')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"a266a240-4d48-4381-84aa-fb0150b5706c","_cell_guid":"cbc856e3-0d7a-42f3-941e-397019507eb3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.749465Z","iopub.execute_input":"2025-07-02T02:33:25.749805Z","iopub.status.idle":"2025-07-02T02:33:25.772851Z","shell.execute_reply.started":"2025-07-02T02:33:25.749775Z","shell.execute_reply":"2025-07-02T02:33:25.772083Z"},"jupyter":{"outputs_hidden":false},"id":"bFBMI6kKskSG"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def analyze_weight_statistics(weight_history):\n    \"\"\"Analyze statistical properties of weight evolution\"\"\"\n    kan_weights = [w['kan_weight'] for w in weight_history]\n    garch_weights = [w['garch_weight'] for w in weight_history]\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"WEIGHT STATISTICS ANALYSIS\")\n    print(\"=\"*50)\n    print(f\"KAN Weight - Mean: {np.mean(kan_weights):.4f}, Std: {np.std(kan_weights):.4f}\")\n    print(f\"GARCH Weight - Mean: {np.mean(garch_weights):.4f}, Std: {np.std(garch_weights):.4f}\")\n    print(f\"Weight Correlation: {np.corrcoef(kan_weights, garch_weights)[0,1]:.4f}\")\n\n    # Check convergence\n    final_weights = weight_history[-10:]  # Last 10 epochs\n    final_kan_std = np.std([w['kan_weight'] for w in final_weights])\n    print(f\"Final KAN weight stability (std of last 10 epochs): {final_kan_std:.4f}\")\n\n    if final_kan_std < 0.05:\n        print(\"✓ Weights have converged (low variance in final epochs)\")\n    else:\n        print(\"⚠ Weights may still be adapting (high variance in final epochs)\")","metadata":{"_uuid":"78ec8dd9-be40-430d-b1d9-0b57b79fd0a7","_cell_guid":"979ae676-d276-4557-ae9b-7821b5006dcb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.773890Z","iopub.execute_input":"2025-07-02T02:33:25.774704Z","iopub.status.idle":"2025-07-02T02:33:25.796821Z","shell.execute_reply.started":"2025-07-02T02:33:25.774679Z","shell.execute_reply":"2025-07-02T02:33:25.795815Z"},"jupyter":{"outputs_hidden":false},"id":"kgpXcCb2skSG"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def component_contribution_analysis(hybrid_model, X_test, X_garch_test, scaler_y):\n    \"\"\"Analyze individual component contributions\"\"\"\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPONENT CONTRIBUTION ANALYSIS\")\n    print(\"=\"*60)\n\n    hybrid_model.eval()\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    X_garch_test_tensor = torch.tensor(X_garch_test, dtype=torch.float32)\n\n    with torch.no_grad():\n        # Get individual component outputs\n        kan_output = hybrid_model.kan_component(X_test_tensor)\n        garch_output = hybrid_model.garch_component(X_garch_test_tensor)\n\n        # Get fusion output\n        combined = torch.cat([kan_output, garch_output], dim=1)\n        fusion_output = hybrid_model.fusion(combined)\n\n        # Get weighted combination\n        weighted_output, weight_info = hybrid_model.weight_manager(kan_output, garch_output)\n\n        # Convert to numpy and inverse transform\n        kan_pred = scaler_y.inverse_transform(kan_output.mean(dim=1, keepdim=True).numpy())\n        garch_pred = scaler_y.inverse_transform(garch_output.numpy())\n        fusion_pred = scaler_y.inverse_transform(fusion_output.numpy())\n        weighted_pred = scaler_y.inverse_transform(weighted_output.numpy())\n\n        print(f\"Current weights:\")\n        print(f\"  KAN weight: {weight_info['kan_weight'].item():.4f}\")\n        print(f\"  GARCH weight: {weight_info['garch_weight'].item():.4f}\")\n\n        print(f\"\\nComponent statistics (last 100 predictions):\")\n        print(f\"  KAN - Mean: {np.mean(kan_pred[-100:]):.4f}, Std: {np.std(kan_pred[-100:]):.4f}\")\n        print(f\"  GARCH - Mean: {np.mean(garch_pred[-100:]):.4f}, Std: {np.std(garch_pred[-100:]):.4f}\")\n        print(f\"  Fusion - Mean: {np.mean(fusion_pred[-100:]):.4f}, Std: {np.std(fusion_pred[-100:]):.4f}\")\n        print(f\"  Weighted - Mean: {np.mean(weighted_pred[-100:]):.4f}, Std: {np.std(weighted_pred[-100:]):.4f}\")\n\n        # Component correlation analysis\n        kan_flat = kan_pred.flatten()\n        garch_flat = garch_pred.flatten()\n        fusion_flat = fusion_pred.flatten()\n\n        kan_garch_corr = np.corrcoef(kan_flat, garch_flat)[0, 1]\n        kan_fusion_corr = np.corrcoef(kan_flat, fusion_flat)[0, 1]\n        garch_fusion_corr = np.corrcoef(garch_flat, fusion_flat)[0, 1]\n\n        print(f\"\\nComponent correlations:\")\n        print(f\"  KAN-GARCH: {kan_garch_corr:.4f}\")\n        print(f\"  KAN-Fusion: {kan_fusion_corr:.4f}\")\n        print(f\"  GARCH-Fusion: {garch_fusion_corr:.4f}\")\n\n        return {\n            'kan_predictions': kan_pred,\n            'garch_predictions': garch_pred,\n            'fusion_predictions': fusion_pred,\n            'weighted_predictions': weighted_pred,\n            'weight_info': weight_info\n        }\n","metadata":{"_uuid":"1d87f300-9b35-440e-b726-0c4913ce3daa","_cell_guid":"f3c20a62-4504-4130-a754-9584b2dd7643","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.797812Z","iopub.execute_input":"2025-07-02T02:33:25.798194Z","iopub.status.idle":"2025-07-02T02:33:25.824078Z","shell.execute_reply.started":"2025-07-02T02:33:25.798152Z","shell.execute_reply":"2025-07-02T02:33:25.823117Z"},"jupyter":{"outputs_hidden":false},"id":"U786HUqZskSG"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def volatility_regime_analysis(predictions, y_true, threshold_percentiles=[25, 75]):\n    \"\"\"Analyze model performance across different volatility regimes\"\"\"\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"VOLATILITY REGIME ANALYSIS\")\n    print(\"=\"*60)\n\n    # Define volatility regimes based on percentiles\n    low_threshold = np.percentile(y_true, threshold_percentiles[0])\n    high_threshold = np.percentile(y_true, threshold_percentiles[1])\n\n    # Classify into regimes\n    low_vol_mask = y_true <= low_threshold\n    normal_vol_mask = (y_true > low_threshold) & (y_true <= high_threshold)\n    high_vol_mask = y_true > high_threshold\n\n    regimes = {\n        'Low Volatility': (low_vol_mask, f'≤ {low_threshold:.4f}'),\n        'Normal Volatility': (normal_vol_mask, f'{low_threshold:.4f} - {high_threshold:.4f}'),\n        'High Volatility': (high_vol_mask, f'> {high_threshold:.4f}')\n    }\n\n    print(f\"Regime thresholds:\")\n    print(f\"  Low: {regimes['Low Volatility'][1]}\")\n    print(f\"  Normal: {regimes['Normal Volatility'][1]}\")\n    print(f\"  High: {regimes['High Volatility'][1]}\")\n    print()\n\n    regime_metrics = {}\n\n    for regime_name, (mask, range_str) in regimes.items():\n        if np.sum(mask) > 0:  # Ensure we have data points\n            regime_true = y_true[mask]\n            regime_pred = predictions[mask]\n\n            rmse = np.sqrt(mean_squared_error(regime_true, regime_pred))\n            mae = mean_absolute_error(regime_true, regime_pred)\n            r2 = r2_score(regime_true, regime_pred)\n\n            regime_metrics[regime_name] = {\n                'count': np.sum(mask),\n                'rmse': rmse,\n                'mae': mae,\n                'r2': r2,\n                'range': range_str\n            }\n\n            print(f\"{regime_name} ({np.sum(mask)} points):\")\n            print(f\"  RMSE: {rmse:.6f}\")\n            print(f\"  MAE: {mae:.6f}\")\n            print(f\"  R²: {r2:.4f}\")\n            print()\n\n    return regime_metrics","metadata":{"_uuid":"2af937cd-dde5-421f-ab09-0d32142b3af3","_cell_guid":"0c68b8f1-3ca0-4706-9c01-992545ee5b82","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.825589Z","iopub.execute_input":"2025-07-02T02:33:25.825930Z","iopub.status.idle":"2025-07-02T02:33:25.847849Z","shell.execute_reply.started":"2025-07-02T02:33:25.825892Z","shell.execute_reply":"2025-07-02T02:33:25.846844Z"},"jupyter":{"outputs_hidden":false},"id":"sO76bkYAskSG"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def residual_analysis(predictions, y_true):\n    \"\"\"Perform comprehensive residual analysis\"\"\"\n\n    residuals = y_true - predictions\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"RESIDUAL ANALYSIS\")\n    print(\"=\"*60)\n\n    # Basic statistics\n    print(f\"Residual statistics:\")\n    print(f\"  Mean: {np.mean(residuals):.6f}\")\n    print(f\"  Std: {np.std(residuals):.6f}\")\n    print(f\"  Skewness: {np.mean(((residuals - np.mean(residuals)) / np.std(residuals))**3):.4f}\")\n    print(f\"  Kurtosis: {np.mean(((residuals - np.mean(residuals)) / np.std(residuals))**4):.4f}\")\n\n    # Autocorrelation\n    if len(residuals) > 1:\n        autocorr = np.corrcoef(residuals[1:], residuals[:-1])[0, 1]\n        print(f\"  Lag-1 autocorrelation: {autocorr:.4f}\")\n\n        if abs(autocorr) > 0.1:\n            print(\"  ⚠ Significant autocorrelation detected\")\n        else:\n            print(\"  ✓ Low autocorrelation in residuals\")\n\n    # Plot residuals\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # Residuals vs predictions\n    axes[0, 0].scatter(predictions, residuals, alpha=0.6, s=20)\n    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n    axes[0, 0].set_xlabel('Predictions')\n    axes[0, 0].set_ylabel('Residuals')\n    axes[0, 0].set_title('Residuals vs Predictions')\n    axes[0, 0].grid(True, alpha=0.3)\n\n    # Residual histogram\n    axes[0, 1].hist(residuals, bins=30, alpha=0.7, density=True)\n    axes[0, 1].set_xlabel('Residuals')\n    axes[0, 1].set_ylabel('Density')\n    axes[0, 1].set_title('Residual Distribution')\n    axes[0, 1].grid(True, alpha=0.3)\n\n    # Residuals over time\n    axes[1, 0].plot(residuals, alpha=0.7)\n    axes[1, 0].axhline(y=0, color='red', linestyle='--')\n    axes[1, 0].set_xlabel('Time')\n    axes[1, 0].set_ylabel('Residuals')\n    axes[1, 0].set_title('Residuals Over Time')\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # Residual autocorrelation plot\n    if len(residuals) > 10:\n        lags = range(1, min(20, len(residuals)//2))\n        autocorrs = [np.corrcoef(residuals[lag:], residuals[:-lag])[0, 1] for lag in lags]\n        axes[1, 1].bar(lags, autocorrs, alpha=0.7)\n        axes[1, 1].axhline(y=0, color='red', linestyle='-')\n        axes[1, 1].axhline(y=0.05, color='red', linestyle='--', alpha=0.5)\n        axes[1, 1].axhline(y=-0.05, color='red', linestyle='--', alpha=0.5)\n        axes[1, 1].set_xlabel('Lag')\n        axes[1, 1].set_ylabel('Autocorrelation')\n        axes[1, 1].set_title('Residual Autocorrelation')\n        axes[1, 1].grid(True, alpha=0.3)\n    else:\n        axes[1, 1].text(0.5, 0.5, 'Not enough data\\nfor autocorrelation',\n                       ha='center', va='center', transform=axes[1, 1].transAxes)\n        axes[1, 1].set_title('Autocorrelation (Insufficient Data)')\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        'mean': np.mean(residuals),\n        'std': np.std(residuals),\n        'autocorr': autocorr if len(residuals) > 1 else 0\n    }","metadata":{"_uuid":"c99c99e4-8f2e-4910-b395-76ab4336b555","_cell_guid":"228182ad-d790-4f37-92e5-270d0ee3b0c1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.848920Z","iopub.execute_input":"2025-07-02T02:33:25.849227Z","iopub.status.idle":"2025-07-02T02:33:25.877914Z","shell.execute_reply.started":"2025-07-02T02:33:25.849198Z","shell.execute_reply":"2025-07-02T02:33:25.876974Z"},"jupyter":{"outputs_hidden":false},"id":"Fd7IlFZ1skSH"},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def analyze_model_components(hybrid_model, X_test, X_garch_test):\n    \"\"\"Analyze the contribution of different components in the hybrid model\"\"\"\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"HYBRID MODEL COMPONENT ANALYSIS\")\n    print(\"=\"*50)\n\n    hybrid_model.eval()\n    with torch.no_grad():\n        # Get KAN component output\n        kan_output = hybrid_model.kan_component(torch.tensor(X_test, dtype=torch.float32))\n\n        # Get GARCH component output\n        garch_output = hybrid_model.garch_component(torch.tensor(X_garch_test, dtype=torch.float32))\n\n        # Get learned weights\n        kan_weight = torch.sigmoid(hybrid_model.kan_weight).item()\n        garch_weight = torch.sigmoid(hybrid_model.garch_weight).item()\n\n        print(f\"Learned KAN weight: {kan_weight:.4f}\")\n        print(f\"Learned GARCH weight: {garch_weight:.4f}\")\n        print(f\"Weight ratio (KAN/GARCH): {kan_weight/garch_weight:.4f}\")\n\n        # Analyze component correlations\n        kan_flat = kan_output.mean(dim=1).numpy()\n        garch_flat = garch_output.flatten().numpy()\n\n        correlation = np.corrcoef(kan_flat, garch_flat)[0, 1]\n        print(f\"Correlation between KAN and GARCH components: {correlation:.4f}\")\n\n        return kan_output.numpy(), garch_output.numpy(), kan_weight, garch_weight","metadata":{"_uuid":"ae51b0ba-c6f8-4294-a4fa-0e5200fa2050","_cell_guid":"cc7c2712-66ea-4c6e-acda-b68719f28efd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.879056Z","iopub.execute_input":"2025-07-02T02:33:25.879444Z","iopub.status.idle":"2025-07-02T02:33:25.903028Z","shell.execute_reply.started":"2025-07-02T02:33:25.879409Z","shell.execute_reply":"2025-07-02T02:33:25.902014Z"},"jupyter":{"outputs_hidden":false},"id":"JyrqyWKcskSH"},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def feature_importance_analysis(model, X_test, feature_names=None):\n    \"\"\"Simple feature importance analysis using permutation\"\"\"\n\n    if feature_names is None:\n        feature_names = [f\"Feature_{i}\" for i in range(X_test.shape[1])]\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"FEATURE IMPORTANCE ANALYSIS (Top 10)\")\n    print(\"=\"*50)\n\n    model.eval()\n    X_tensor = torch.tensor(X_test, dtype=torch.float32)\n\n    with torch.no_grad():\n        baseline_pred = model(X_tensor).numpy()\n        baseline_mse = np.mean(baseline_pred**2)\n\n    importance_scores = []\n\n    for i in range(X_test.shape[1]):\n        X_permuted = X_test.copy()\n        # Permute feature i\n        np.random.shuffle(X_permuted[:, i])\n        X_perm_tensor = torch.tensor(X_permuted, dtype=torch.float32)\n\n        with torch.no_grad():\n            perm_pred = model(X_perm_tensor).numpy()\n            perm_mse = np.mean(perm_pred**2)\n\n        importance = perm_mse - baseline_mse\n        importance_scores.append((feature_names[i], importance))\n\n    # Sort by importance\n    importance_scores.sort(key=lambda x: x[1], reverse=True)\n\n    print(f\"{'Feature':<25} {'Importance':<15}\")\n    print(\"-\" * 40)\n    for name, score in importance_scores[:10]:\n        print(f\"{name:<25} {score:<15.6f}\")","metadata":{"_uuid":"5e7c4e83-df8b-4abc-ab4b-227a5e21d967","_cell_guid":"10a07c0f-8561-4b14-b23b-2b5670eff7b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.903986Z","iopub.execute_input":"2025-07-02T02:33:25.904490Z","iopub.status.idle":"2025-07-02T02:33:25.924715Z","shell.execute_reply.started":"2025-07-02T02:33:25.904461Z","shell.execute_reply":"2025-07-02T02:33:25.923869Z"},"jupyter":{"outputs_hidden":false},"id":"6yCnHjOeskSH"},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def main():\n    \"\"\"Main function to run the complete volatility prediction pipeline\"\"\"\n    \n    print(\"Enhanced Volatility Prediction: KAN, MLP, GARCH, and Hybrid Models\")\n    print(\"=\" * 70)\n    \n    # Load your data here - replace with your actual data path\n    # Try to load the data (you may need to adjust this path)\n    data = pd.read_csv('/kaggle/input/fptdata/fpt.csv', index_col=False)\n    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n    print(f\"Loaded data: {data.shape}\")\n    print(f\"Columns: {list(data.columns)}\")\n    \n    # Set volatility windows\n    vol_windows = [30, 40, 50, 60]\n    lookback = 30\n    all_results = {}\n    \n    for win in vol_windows:\n        print(f\"\\nUsing volatility windows: {win}\")\n        print(f\"Lookback period: {lookback}\")\n        \n        # Train all models\n        results, y_train, y_test_aligned, scalers = train_model_with_performance_weights(data, lookback, win)\n        \n        # Evaluate models\n        model_metrics = evaluate_models(results, y_test_aligned)\n\n        all_results[f'window_{win}'] = {\n            'results': results,\n            'metrics': model_metrics,\n            'scalers': scalers,\n            'y_test': y_test_aligned\n        }\n        \n        # Create comprehensive plots\n        create_comprehensive_plots(results, model_metrics, y_train, y_test_aligned)\n        \n        # Print final summary\n        print(\"\\n\" + \"=\"*70)\n        print(f\"SUMMARY FOR WINDOW {win}\")\n        print(\"=\"*70)\n        \n        # Find overall best model\n        best_overall = min(model_metrics.items(), key=lambda x: x[1]['rmse'])\n        print(f\"• Best overall model: {best_overall[0].upper()} (lowest RMSE)\")\n        \n        # Check if hybrid is better than individual models\n        if 'hybrid' in model_metrics and 'kan' in model_metrics and 'garch' in model_metrics and 'mlp' in model_metrics:\n            hybrid_rmse = model_metrics['hybrid']['rmse']\n            kan_rmse = model_metrics['kan']['rmse']\n            garch_rmse = model_metrics['garch']['rmse']\n            mlp_rmse = model_metrics['mlp']['rmse']\n            \n            individual_models = [kan_rmse, garch_rmse, mlp_rmse]\n            best_individual_rmse = min(individual_models)\n            \n            if hybrid_rmse < best_individual_rmse:\n                print(\"• Hybrid model outperforms all individual models\")\n            else:\n                print(\"• Some individual models perform better than hybrid\")\n                \n            # Compare KAN vs MLP\n            if kan_rmse < mlp_rmse:\n                print(f\"• KAN outperforms MLP (RMSE: {kan_rmse:.6f} vs {mlp_rmse:.6f})\")\n            else:\n                print(f\"• MLP outperforms KAN (RMSE: {mlp_rmse:.6f} vs {kan_rmse:.6f})\")\n\n    print(f\"\\n{'='*80}\")\n    print(\"COMPARISON ACROSS ALL WINDOWS\")\n    print(f\"{'='*80}\")\n    \n    for window_key, window_data in all_results.items():\n        window_num = window_key.split('_')[1]\n        metrics = window_data['metrics']\n        best_model = min(metrics.items(), key=lambda x: x[1]['rmse'])\n        print(f\"Window {window_num}: Best = {best_model[0].upper()} (RMSE: {best_model[1]['rmse']:.6f}, R²: {best_model[1]['r2']:.4f})\")\n    \n    return all_results","metadata":{"_uuid":"cc296633-c9a4-4980-87f7-53624af0af71","_cell_guid":"a409b522-64c0-4fa1-88f4-86c7ac152b54","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.925647Z","iopub.execute_input":"2025-07-02T02:33:25.925880Z","iopub.status.idle":"2025-07-02T02:33:25.945801Z","shell.execute_reply.started":"2025-07-02T02:33:25.925861Z","shell.execute_reply":"2025-07-02T02:33:25.944981Z"},"jupyter":{"outputs_hidden":false},"id":"gEw1AtZNskSH"},"outputs":[],"execution_count":22},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    all_results = main()","metadata":{"_uuid":"f13422f7-6a00-42ff-8d9c-0170e28b04af","_cell_guid":"1b763727-27a8-418b-8852-9567c8688044","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T02:33:25.946651Z","iopub.execute_input":"2025-07-02T02:33:25.946894Z"},"id":"M4DihMriskSH","jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Enhanced Volatility Prediction: KAN, MLP, GARCH, and Hybrid Models\n======================================================================\nLoaded data: (2497, 6)\nColumns: ['Date', 'Close', 'Adjusted', 'Open', 'High', 'Low']\n\nUsing volatility windows: 30\nLookback period: 30\nCreating volatility features...\nFeature matrix shape: (2452, 17)\nGARCH feature matrix shape: (2452, 3)\nTarget vector shape: (2452,)\n\n==================================================\nTRAINING MODELS\n==================================================\n\n1. Training KAN Model...\nKAN Epoch 0: Train=0.979341, Test=0.556148\n","output_type":"stream"}],"execution_count":null}]}